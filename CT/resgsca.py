import math
import os
import glob
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler
from torchvision import transforms
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import KFold, StratifiedKFold
from tqdm import tqdm
from collections import Counter
import torch_optimizer as optim
import pandas as pd
import time
import matplotlib.pyplot as plt
import cv2

def generate_attention_visualization(model, image_tensor, original_image=None, save_path=None, alpha=0.5):
    """
    ç”Ÿæˆæ³¨æ„åŠ›å¯è§†åŒ–å›¾å¹¶ä¸åŸå§‹å›¾åƒå åŠ 
    
    å‚æ•°:
        model: CTModelå®ä¾‹
        image_tensor: è¾“å…¥å¼ é‡ [1, 3, H, W]
        original_image: åŸå§‹å›¾åƒnumpyæ•°ç»„ (H, W, 3)ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨tensor
        save_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä¿å­˜
        alpha: æ³¨æ„åŠ›å›¾ä¸åŸå§‹å›¾åƒçš„æ··åˆæ¯”ä¾‹
    
    è¿”å›:
        overlaid_image: å åŠ äº†æ³¨æ„åŠ›å›¾çš„åŸå§‹å›¾åƒ
        attention_maps: æ‰€æœ‰æ³¨æ„åŠ›å›¾
    """
    # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    model.eval()
    # ç”Ÿæˆæ³¨æ„åŠ›å›¾
    with torch.no_grad():
        attention_maps = model.visualize_attention(image_tensor.unsqueeze(0))
    # åˆ›å»ºå åŠ å›¾åƒ
    overlaid_images = []
    for i, attention_map in enumerate(attention_maps):
        # è½¬æ¢æ³¨æ„åŠ›å›¾ä¸ºçƒ­åŠ›å›¾
        attention_map = attention_map.squeeze().cpu().numpy()
        heatmap = cv2.applyColorMap(np.uint8(255 * attention_map), cv2.COLORMAP_JET)
        
        # è½¬æ¢ä¸ºRGBï¼ˆå¦‚æœéœ€è¦ï¼‰
        if len(heatmap.shape) == 2:
            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_GRAY2RGB)
        elif heatmap.shape[2] == 3:
            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
        
        # å½’ä¸€åŒ–çƒ­åŠ›å›¾
        heatmap = heatmap.astype(float) / 255
        
        # ç¡®ä¿åŸå§‹å›¾åƒæ˜¯floatç±»å‹ä¸”åœ¨0-1èŒƒå›´å†…
        if original_image.dtype != np.float32:
            original_image = original_image.astype(float) / 255
        
        # å åŠ å›¾åƒ
        overlaid = cv2.addWeighted(original_image, 1-alpha, heatmap, alpha, 0)
        overlaid_images.append(overlaid)
    
    # å¦‚æœéœ€è¦ä¿å­˜å›¾åƒ
    if save_path is not None:
        # åˆ›å»ºå›¾åƒç½‘æ ¼
        n_maps = len(overlaid_images)
        n_cols = min(4, n_maps)  # æ¯è¡Œæœ€å¤š4å¼ å›¾
        n_rows = (n_maps + n_cols - 1) // n_cols
        
        plt.figure(figsize=(4*n_cols, 4*n_rows))
        for i, img in enumerate(overlaid_images):
            plt.subplot(n_rows, n_cols, i+1)
            plt.imshow(img)
            plt.title(f'Attention Layer {i+1}')
            plt.axis('off')
        
        plt.tight_layout()
        plt.savefig(save_path)
        plt.close()
    
    return overlaid_images, attention_maps

class FocalLoss(nn.Module):
    def __init__(self,
                 alpha=None,
                 gamma: float = 2.0,
                 reduction: str = 'mean',
                 ignore_index: int = -100):
        """
        alpha:         None æˆ–è€… shape=[num_classes] çš„ Tensor æˆ– floatï¼Œ
                       äºŒåˆ†ç±»æ—¶ float ä¼šè¢«è½¬æ¢ä¸º [alpha, 1-alpha] Tensor
        gamma:         èšç„¦ç³»æ•° Î³ï¼Œå…¸å‹å€¼ 2.0
        reduction:     'none' | 'mean' | 'sum'
        ignore_index:  å¿½ç•¥æ ‡ç­¾
        """
        super().__init__()
        # å¦‚æœæ˜¯å•ä¸€ floatï¼ˆç”¨äºäºŒåˆ†ç±»ï¼‰ï¼Œè½¬æ¢ä¸º [Î±, 1âˆ’Î±] Tensor
        if isinstance(alpha, float):
            self.alpha = torch.tensor([alpha, 1.0 - alpha], dtype=torch.float32)
        else:
            self.alpha = alpha  # None æˆ– å·²ç»æ˜¯ Tensor
        self.gamma = gamma
        self.reduction = reduction
        self.ignore_index = ignore_index

    def forward(self, inputs, targets):
        """
        inputs:  [N, C] logits æˆ–è€… [N] logitsï¼ˆäºŒåˆ†ç±»ï¼‰
        targets: [N] LongTensor
        """
        # äºŒåˆ†ç±»åˆ†æ”¯
        if inputs.dim() == 1 or inputs.size(1) == 1:
            logits = inputs.view(-1)
            targets = targets.view(-1).float()
            bce_loss = F.binary_cross_entropy_with_logits(
                logits, targets, reduction='none')
            pt = torch.exp(-bce_loss)  # pt = sigmoid(logit) æˆ– 1âˆ’pt
            if self.alpha is not None:
                # å¯¹äºŒåˆ†ç±» alpha å¼ é‡å¹¿æ’­
                alpha_factor = (targets * self.alpha[0] +
                                (1 - targets) * self.alpha[1]).to(bce_loss.device)
                loss = alpha_factor * (1 - pt) ** self.gamma * bce_loss
            else:
                loss = (1 - pt) ** self.gamma * bce_loss

        # å¤šåˆ†ç±»åˆ†æ”¯
        else:
            logp = F.log_softmax(inputs, dim=1)     # [N, C] :contentReference[oaicite:0]{index=0}
            p = torch.exp(logp)                     # [N, C]
            # å–å¯¹åº”ç±»åˆ«çš„ log p_t å’Œ p_t
            targets = targets.view(-1, 1)
            logpt = logp.gather(1, targets).view(-1)
            pt = p.gather(1, targets).view(-1)
            # å‡†å¤‡ Î±_t
            if self.alpha is not None:
                # ç¡®ä¿ Î± æ˜¯ Tensor å¹¶åœ¨åŒ device
                if not isinstance(self.alpha, torch.Tensor):
                    raise ValueError("alpha must be Tensor for multiclass")
                at = self.alpha.to(inputs.device).gather(0, targets.view(-1))
            else:
                at = 1.0
            loss = -at * (1 - pt) ** self.gamma * logpt

        # å¿½ç•¥ç‰¹å®šæ ‡ç­¾
        if self.ignore_index >= 0:
            valid = targets.view(-1) != self.ignore_index
            loss = loss[valid]

        # Reduction
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss
        
class ChannelAttention2D(nn.Module):
    def __init__(self, inplanes, reduction=16):
        super().__init__()
        self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)
        self.softmax = nn.Softmax(dim=2)
        self.channel_mul_conv = nn.Sequential(
            nn.Conv2d(inplanes, inplanes // reduction, 1),
            nn.LayerNorm([inplanes // reduction, 1, 1]),
            nn.ReLU(inplace=True),
            nn.Conv2d(inplanes // reduction, inplanes, 1),
            nn.LayerNorm([inplanes, 1, 1])
        )
        self.sigmoid = nn.Sigmoid()

    def spatial_pool(self, x):
        B, C, H, W = x.size()
        input_x = x.view(B, C, -1).unsqueeze(1)              # B Ã— 1 Ã— C Ã— (HÃ—W)
        context_mask = self.conv_mask(x).view(B, 1, -1)      # B Ã— 1 Ã— (HÃ—W)
        context_mask = self.softmax(context_mask).unsqueeze(-1)
        context = torch.matmul(input_x, context_mask)        # B Ã— 1 Ã— C Ã— 1
        return context.view(B, C, 1, 1)

    def forward(self, x):
        context = self.spatial_pool(x)
        channel_mul_term = self.sigmoid(self.channel_mul_conv(context))
        return x * channel_mul_term

class SpatialAttention2D(nn.Module):
    def __init__(self, kernel_size=7):
        super().__init__()
        padding = 3 if kernel_size == 7 else 1
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        avg_out = torch.mean(x, dim=1, keepdim=True)
        x_cat = torch.cat([max_out, avg_out], dim=1)
        return x * self.sigmoid(self.conv1(x_cat))

class GCSAM2D(nn.Module):
    def __init__(self, in_channels, reduction=16):
        super().__init__()
        self.channel_attention = ChannelAttention2D(in_channels, reduction)
        self.spatial_attention = SpatialAttention2D()

    def forward(self, x):
        x = self.channel_attention(x)
        x = self.spatial_attention(x)
        return x

class Bottle2neck2D(nn.Module):
    expansion = 1
    def __init__(self, inplanes, planes, stride=1, baseWidth=26, scale=4, stype='normal'):
        super().__init__()
        width = int(math.floor(inplanes / 4 * (baseWidth / 64.0)))
        self.conv1 = nn.Conv2d(inplanes, width * scale, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(width * scale)
        self.relu = nn.ReLU(inplace=True)

        self.nums = scale - 1 if scale != 1 else 1
        self.stype = stype
        self.scale = scale
        self.width = width

        self.convs = nn.ModuleList([
            nn.Conv2d(width, width, kernel_size=3, stride=1, padding=1, bias=False)
            for _ in range(self.nums)
        ])
        self.bns = nn.ModuleList([
            nn.BatchNorm2d(width)
            for _ in range(self.nums)
        ])

        self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1)
        self.conv3 = nn.Conv2d(width * scale, planes * self.expansion, kernel_size=1, stride=stride, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)

        if inplanes != planes or stride != 1:
            self.shortcut = nn.Sequential(
                nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes)
            )
        else:
            self.shortcut = nn.Identity()

        self.GCS = GCSAM2D(planes)

    def forward(self, x):
        residual = x
        out = self.relu(self.bn1(self.conv1(x)))
        spx = torch.split(out, self.width, 1)

        for i in range(self.nums):
            sp = spx[i] if i == 0 or self.stype == 'stage' else sp + spx[i]
            sp = self.relu(self.bns[i](self.convs[i](sp)))
            out = sp if i == 0 else torch.cat((out, sp), 1)

        if self.scale != 1:
            if self.stype == 'normal':
                out = torch.cat((out, spx[self.nums]), 1)
            elif self.stype == 'stage':
                out = torch.cat((out, self.pool(spx[self.nums])), 1)

        out = self.bn3(self.conv3(out))
        residual = self.shortcut(residual)
        out = self.GCS(out)
        return self.relu(out + residual)

class Encoder2D(nn.Module):
    def __init__(self, channels, dropout_prob=0.3):
        super().__init__()
        self.blocks = nn.ModuleList([
            nn.Sequential(
                Bottle2neck2D(channels[i], channels[i+1]),
                Bottle2neck2D(channels[i+1], channels[i+1]),
                nn.MaxPool2d(kernel_size=2, stride=2),
                nn.Dropout(p=dropout_prob)
            ) for i in range(len(channels)-1)
        ])

    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        return x

class CTModel(nn.Module):
    def __init__(self, num_classes=2):
        super().__init__()
        
        self.preBlock = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(kernel_size=2, stride=2)
        )
        
        self.encoder = Encoder2D([128, 256, 256])

        self.gap = nn.Sequential(
            #nn.Conv2d(512, 256, kernel_size=1),
            nn.AdaptiveAvgPool2d((8, 8)),
            nn.Flatten(),
            nn.LayerNorm(256*8*8)
        )
        
        self.mlp = nn.Sequential(
            nn.Linear(256*8*8, 128),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )

    def forward(self, x, return_attention=False):
        """
        å‰å‘ä¼ æ’­ï¼Œæ”¯æŒæ³¨æ„åŠ›å¯è§†åŒ–
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ï¼Œå¯ä»¥æ˜¯:
               - æ ‡å‡†å›¾åƒå¼ é‡ [B, 3, H, W]
               - å•é€šé“CTå›¾åƒ [B, 1, H, W]
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›å›¾
        """
        # å¤„ç†å•é€šé“è¾“å…¥
        if x.size(1) == 1:
            x = x.repeat(1, 3, 1, 1)  # å°†å•é€šé“æ‰©å±•ä¸º3é€šé“
            
        x = self.preBlock(x)
        
        # å­˜å‚¨æ³¨æ„åŠ›å›¾
        attention_maps = []
        
        # åœ¨encoderä¸­è·å–æ³¨æ„åŠ›å›¾
        for block in self.encoder.blocks:
            # è·å–Bottle2neck2Dçš„GCSAMæ³¨æ„åŠ›
            bottle_block = block[0]  # ç¬¬ä¸€ä¸ªBottle2neck2D
            
            # åº”ç”¨ç¬¬ä¸€ä¸ªå·ç§¯å±‚
            feat = bottle_block.relu(bottle_block.bn1(bottle_block.conv1(x)))
            spx = torch.split(feat, bottle_block.width, 1)
            
            # å¤„ç†splitç‰¹å¾
            out = None
            for i in range(bottle_block.nums):
                sp = spx[i] if i == 0 or bottle_block.stype == 'stage' else sp + spx[i]
                sp = bottle_block.relu(bottle_block.bns[i](bottle_block.convs[i](sp)))
                out = sp if i == 0 else torch.cat((out, sp), 1)
            
            if bottle_block.scale != 1:
                if bottle_block.stype == 'normal':
                    out = torch.cat((out, spx[bottle_block.nums]), 1)
                elif bottle_block.stype == 'stage':
                    out = torch.cat((out, bottle_block.pool(spx[bottle_block.nums])), 1)
            
            # è·å–æ³¨æ„åŠ›å›¾
            attention_feat = bottle_block.bn3(bottle_block.conv3(out))
            channel_attention = bottle_block.GCS.channel_attention(attention_feat)
            spatial_attention = bottle_block.GCS.spatial_attention(channel_attention)
            
            # å­˜å‚¨æ³¨æ„åŠ›å›¾
            attention_maps.append(spatial_attention)
            
            # æ­£å¸¸å‰å‘ä¼ æ’­
            x = block(x)
        
        x = self.gap(x)
        x = self.mlp(x)
        
        if return_attention:
            return x, attention_maps
        return x

    def visualize_attention(self, x, original_size=None):
        """
        ç”Ÿæˆæ³¨æ„åŠ›å¯è§†åŒ–å›¾
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡
            original_size: åŸå§‹å›¾ç‰‡å¤§å°ï¼Œç”¨äºä¸Šé‡‡æ · (H, W)
        
        è¿”å›:
            attention_visualizations: æ³¨æ„åŠ›å¯è§†åŒ–å›¾åˆ—è¡¨
        """
        _, attention_maps = self.forward(x, return_attention=True)
        attention_visualizations = []
        
        for attention_map in attention_maps:
            # æå–ç©ºé—´æ³¨æ„åŠ›æƒé‡
            spatial_weights = attention_map.mean(1, keepdim=True)  # [B, 1, H, W]
            
            # å½’ä¸€åŒ–åˆ°[0, 1]èŒƒå›´
            spatial_weights = (spatial_weights - spatial_weights.min()) / (spatial_weights.max() - spatial_weights.min() + 1e-8)
            
            # å¦‚æœéœ€è¦ï¼Œè°ƒæ•´å¤§å°åˆ°åŸå§‹å›¾ç‰‡å°ºå¯¸
            if original_size is not None:
                spatial_weights = F.interpolate(spatial_weights, size=original_size, mode='bilinear', align_corners=False)
            
            attention_visualizations.append(spatial_weights)
        
        return attention_visualizations
# ---- Training & Evaluation ----
def cross_validation(dataset, model_class, num_folds=10, epochs=100, batch_size=64, 
                    criterion=None, optimizer_fn=None, scheduler_fn=None, 
                    device=None, save_dir='./cv_checkpoints', 
                    verbose=True, stratified=True, visualize_attention=False):
    """
    æ‰§è¡ŒKæŠ˜äº¤å‰éªŒè¯
    
    å‚æ•°:
        dataset: æ•°æ®é›†
        model_class: æ¨¡å‹ç±»ï¼Œç”¨äºåˆ›å»ºæ–°çš„æ¨¡å‹å®ä¾‹
        num_folds: æŠ˜æ•°ï¼Œé»˜è®¤ä¸º10
        epochs: æ¯æŠ˜è®­ç»ƒçš„è½®æ•°
        batch_size: æ‰¹é‡å¤§å°
        criterion: æŸå¤±å‡½æ•°
        optimizer_fn: ä¼˜åŒ–å™¨å‡½æ•°ï¼Œæ¥å—model.parameters()ä½œä¸ºå‚æ•°
        scheduler_fn: å­¦ä¹ ç‡è°ƒåº¦å™¨å‡½æ•°ï¼Œæ¥å—optimizerä½œä¸ºå‚æ•°
        device: è®­ç»ƒè®¾å¤‡
        save_dir: ä¿å­˜æ¨¡å‹çš„ç›®å½•
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        stratified: æ˜¯å¦ä½¿ç”¨åˆ†å±‚æŠ½æ ·
    
    è¿”å›:
        ç»“æœå­—å…¸ï¼ŒåŒ…å«æ¯æŠ˜çš„è¯„ä¼°æŒ‡æ ‡å’Œå¹³å‡æŒ‡æ ‡
    """
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs(save_dir, exist_ok=True)
    
    # å‡†å¤‡äº¤å‰éªŒè¯
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # è·å–æ‰€æœ‰æ ‡ç­¾ç”¨äºåˆ†å±‚æŠ½æ ·
    all_labels = [label for _, label in dataset]
    
    # é€‰æ‹©KæŠ˜äº¤å‰éªŒè¯æ–¹æ³•
    if stratified:
        kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)
        splits = kfold.split(np.arange(len(dataset)), all_labels)
    else:
        kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)
        splits = kfold.split(np.arange(len(dataset)))
    
    # å­˜å‚¨æ¯æŠ˜çš„ç»“æœ
    fold_results = []
    best_models = []
    
    # å¼€å§‹KæŠ˜äº¤å‰éªŒè¯
    for fold, (train_idx, val_idx) in enumerate(splits):
        print(f"\n{'='*20} Fold {fold+1}/{num_folds} {'='*20}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_sampler = SubsetRandomSampler(train_idx)
        val_sampler = SubsetRandomSampler(val_idx)
        
        train_loader = DataLoader(dataset, batch_size=batch_size, 
                                 sampler=train_sampler, num_workers=4)
        val_loader = DataLoader(dataset, batch_size=batch_size,
                               sampler=val_sampler, num_workers=4)
        
        # åˆ›å»ºæ–°çš„æ¨¡å‹å®ä¾‹
        model = model_class().to(device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer = optimizer_fn(model.parameters()) if optimizer_fn else \
                   optim.Lookahead(optim.RAdam(model.parameters(), lr=1e-4))
        scheduler = scheduler_fn(optimizer) if scheduler_fn else \
                   torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        
        # è®­ç»ƒè·Ÿè¸ª
        best_f1 = 0.0
        fold_save_path = os.path.join(save_dir, f'fold_{fold+1}_best.pth')
        fold_metrics = []
        
        # åˆ›å»ºå½“å‰foldçš„å¯è§†åŒ–ä¿å­˜ç›®å½•
        if visualize_attention:
            fold_vis_dir = os.path.join(save_dir, f'fold_{fold+1}_visualizations')
            os.makedirs(fold_vis_dir, exist_ok=True)
        else:
            fold_vis_dir = None

        # è®­ç»ƒå¾ªç¯
        progress_bar = tqdm(range(epochs), desc=f"Fold {fold+1} Training", unit="epoch")
        for epoch in progress_bar:
            train_loss, acc, prec, rec, f1 = train(
                model, train_loader, val_loader, 
                criterion, optimizer, scheduler, device,
                visualize_attention=visualize_attention,
                vis_save_dir=fold_vis_dir,
                current_fold=fold+1,
                current_epoch=epoch+1
            )
            
            # è®°å½•æŒ‡æ ‡
            metrics = {
                'fold': fold + 1,
                'epoch': epoch + 1,
                'train_loss': train_loss,
                'val_acc': acc,
                'val_prec': prec,
                'val_rec': rec,
                'val_f1': f1
            }
            fold_metrics.append(metrics)
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_description(
                f"Fold {fold+1} | Epoch {epoch+1} | Loss: {train_loss:.4f} | "
                f"Acc: {acc:.4f} | F1: {f1:.4f}"
            )
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if f1 > best_f1:
                best_f1 = f1
                torch.save(model.state_dict(), fold_save_path)
                if verbose:
                    print(f"  ğŸ‰ New best F1: {best_f1:.4f}, saved to {fold_save_path}")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
        model.load_state_dict(torch.load(fold_save_path))
        final_acc, final_prec, final_rec, final_f1 = evaluate(model, val_loader, device)
        
        # è®°å½•è¯¥æŠ˜çš„æœ€ç»ˆç»“æœ
        fold_result = {
            'fold': fold + 1,
            'acc': final_acc,
            'prec': final_prec,
            'rec': final_rec,
            'f1': final_f1
        }
        fold_results.append(fold_result)
        best_models.append((model, fold_save_path, final_f1))
        
        print(f"Fold {fold+1} Final Results: Acc={final_acc:.4f}, Prec={final_prec:.4f}, "
              f"Rec={final_rec:.4f}, F1={final_f1:.4f}")
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_acc = sum(r['acc'] for r in fold_results) / num_folds
    avg_prec = sum(r['prec'] for r in fold_results) / num_folds
    avg_rec = sum(r['rec'] for r in fold_results) / num_folds
    avg_f1 = sum(r['f1'] for r in fold_results) / num_folds
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_model_idx = max(range(len(best_models)), key=lambda i: best_models[i][2])
    best_model, best_path, best_f1_score = best_models[best_model_idx]
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°æ€»ä½“æœ€ä½³è·¯å¾„
    overall_best_path = os.path.join(save_dir, 'overall_best.pth')
    torch.save(best_model.state_dict(), overall_best_path)
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*50)
    print(f"Cross-Validation Complete - {num_folds} Folds")
    print(f"Average Metrics: Acc={avg_acc:.4f}, Prec={avg_prec:.4f}, "
          f"Rec={avg_rec:.4f}, F1={avg_f1:.4f}")
    print(f"Best Model from Fold {best_model_idx+1} with F1={best_f1_score:.4f}")
    print(f"Best Model saved to {overall_best_path}")
    print("="*50)
    
    # è¿”å›ç»“æœ
    return {
        'fold_results': fold_results,
        'avg_acc': avg_acc,
        'avg_prec': avg_prec,
        'avg_rec': avg_rec,
        'avg_f1': avg_f1,
        'best_model_fold': best_model_idx + 1,
        'best_model_path': overall_best_path,
        'best_f1': best_f1_score
    }

def evaluate(model, dataloader, device, visualize_attention=False, vis_save_dir=None):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¯é€‰æ‹©æ˜¯å¦å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
    
    å‚æ•°:
        model: æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        visualize_attention: æ˜¯å¦å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
        vis_save_dir: å¯è§†åŒ–ç»“æœä¿å­˜ç›®å½•
    
    è¿”å›:
        acc, prec, rec, f1: è¯„ä¼°æŒ‡æ ‡
    """
    model.eval()
    all_preds, all_labels = [], []
    
    # å¦‚æœéœ€è¦å¯è§†åŒ–ï¼Œç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    if visualize_attention and vis_save_dir:
        os.makedirs(vis_save_dir, exist_ok=True)
    
    with torch.no_grad():
        for i, (imgs, labels) in enumerate(dataloader):
            imgs, labels = imgs.to(device), labels.to(device)
            
            # æ ¹æ®æ˜¯å¦éœ€è¦å¯è§†åŒ–é€‰æ‹©ä¸åŒçš„å‰å‘ä¼ æ’­æ–¹å¼
            if visualize_attention:
                outputs, attention_maps = model(imgs, return_attention=True)
                
                # åªä¸ºå‰å‡ ä¸ªæ ·æœ¬ç”Ÿæˆå¯è§†åŒ–ï¼Œé¿å…ç”Ÿæˆå¤ªå¤šå›¾åƒ
                if i < 5:  # åªå¤„ç†å‰5ä¸ªæ‰¹æ¬¡
                    for j, (img, label) in enumerate(zip(imgs[:4], labels[:4])):  # æ¯æ‰¹æ¬¡åªå¤„ç†å‰4ä¸ªæ ·æœ¬
                        # è·å–åŸå§‹å›¾åƒ
                        original_img = img.cpu().numpy().transpose(1, 2, 0)
                        
                        # ç”Ÿæˆæ³¨æ„åŠ›å¯è§†åŒ–
                        save_path = os.path.join(vis_save_dir, f'batch{i}_sample{j}_class{label.item()}.png')
                        overlaid_images, _ = generate_attention_visualization(
                            model=model,
                            image_tensor=img,
                            original_image=original_img,
                            save_path=save_path,
                            alpha=0.5
                        )
            else:
                outputs = model(imgs)
            
            preds = outputs.argmax(dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    acc = accuracy_score(all_labels, all_preds)
    prec = precision_score(all_labels, all_preds, average='macro', zero_division=0)
    rec = recall_score(all_labels, all_preds, average='macro', zero_division=0)
    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)
    return acc, prec, rec, f1

def train(model, train_loader, val_loader, criterion, optimizer, scheduler, device, 
          visualize_attention=False, vis_save_dir=None, current_fold=None, current_epoch=None):
    """
    è®­ç»ƒä¸€ä¸ªepochå¹¶è¯„ä¼°
    
    å‚æ•°:
        model: æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        device: è®¾å¤‡
        visualize_attention: æ˜¯å¦å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
        vis_save_dir: å¯è§†åŒ–ç»“æœä¿å­˜ç›®å½•
        current_fold: å½“å‰æŠ˜æ•°ï¼ˆç”¨äºäº¤å‰éªŒè¯ï¼‰
        current_epoch: å½“å‰epochæ•°
    """
    model.train()
    total_loss = 0
    for imgs, labels in train_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    scheduler.step()
    
    # åœ¨éªŒè¯æ—¶ç”Ÿæˆå¯è§†åŒ–
    if visualize_attention and vis_save_dir and current_fold is not None and current_epoch is not None:
        # ä¸ºæ¯ä¸ªfoldå’Œepochåˆ›å»ºå•ç‹¬çš„ç›®å½•
        epoch_vis_dir = os.path.join(vis_save_dir, f'fold_{current_fold}', f'epoch_{current_epoch}')
        os.makedirs(epoch_vis_dir, exist_ok=True)
    else:
        epoch_vis_dir = None
    
    val_acc, val_prec, val_rec, val_f1 = evaluate(
        model, val_loader, device,
        visualize_attention=visualize_attention,
        vis_save_dir=epoch_vis_dir
    )
    
    return total_loss / len(train_loader), val_acc, val_prec, val_rec, val_f1

# ---- Dataset ----
class NpzPatchDataset(Dataset):
    def __init__(self, root_dirs, labels_map, transform=None):
        self.samples = []
        self.transform = transform
        for label, dir_path in root_dirs.items():
            for npz_path in glob.glob(os.path.join(dir_path, '*.npz')):
                self.samples.append((npz_path, labels_map[label]))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        npz_path, label = self.samples[idx]
        data = np.load(npz_path)
        img = data['data']  # (H,W,3)
        img = torch.from_numpy(img.transpose(2,0,1)).float() / 255.0
        #img = torch.from_numpy(img).float() / 255.0
        if self.transform:
            img = self.transform(img)
        return img, label
#-----Facol loss------
class FocalLoss(nn.Module):
    def __init__(self,
                 alpha=None,
                 gamma: float = 2.0,
                 reduction: str = 'mean',
                 ignore_index: int = -100):
        """
        alpha:         None æˆ–è€… shape=[num_classes] çš„ Tensor æˆ– floatï¼Œ
                       äºŒåˆ†ç±»æ—¶ float ä¼šè¢«è½¬æ¢ä¸º [alpha, 1-alpha] Tensor
        gamma:         èšç„¦ç³»æ•° Î³ï¼Œå…¸å‹å€¼ 2.0
        reduction:     'none' | 'mean' | 'sum'
        ignore_index:  å¿½ç•¥æ ‡ç­¾
        """
        super().__init__()
        # å¦‚æœæ˜¯å•ä¸€ floatï¼ˆç”¨äºäºŒåˆ†ç±»ï¼‰ï¼Œè½¬æ¢ä¸º [Î±, 1âˆ’Î±] Tensor
        if isinstance(alpha, float):
            self.alpha = torch.tensor([alpha, 1.0 - alpha], dtype=torch.float32)
        else:
            self.alpha = alpha  # None æˆ– å·²ç»æ˜¯ Tensor
        self.gamma = gamma
        self.reduction = reduction
        self.ignore_index = ignore_index

    def forward(self, inputs, targets):
        """
        inputs:  [N, C] logits æˆ–è€… [N] logitsï¼ˆäºŒåˆ†ç±»ï¼‰
        targets: [N] LongTensor
        """
        # äºŒåˆ†ç±»åˆ†æ”¯
        if inputs.dim() == 1 or inputs.size(1) == 1:
            logits = inputs.view(-1)
            targets = targets.view(-1).float()
            bce_loss = F.binary_cross_entropy_with_logits(
                logits, targets, reduction='none')
            pt = torch.exp(-bce_loss)  # pt = sigmoid(logit) æˆ– 1âˆ’pt
            if self.alpha is not None:
                # å¯¹äºŒåˆ†ç±» alpha å¼ é‡å¹¿æ’­
                alpha_factor = (targets * self.alpha[0] +
                                (1 - targets) * self.alpha[1]).to(bce_loss.device)
                loss = alpha_factor * (1 - pt) ** self.gamma * bce_loss
            else:
                loss = (1 - pt) ** self.gamma * bce_loss

        # å¤šåˆ†ç±»åˆ†æ”¯
        else:
            logp = F.log_softmax(inputs, dim=1)     # [N, C] :contentReference[oaicite:0]{index=0}
            p = torch.exp(logp)                     # [N, C]
            # å–å¯¹åº”ç±»åˆ«çš„ log p_t å’Œ p_t
            targets = targets.view(-1, 1)
            logpt = logp.gather(1, targets).view(-1)
            pt = p.gather(1, targets).view(-1)
            # å‡†å¤‡ Î±_t
            if self.alpha is not None:
                # ç¡®ä¿ Î± æ˜¯ Tensor å¹¶åœ¨åŒ device
                if not isinstance(self.alpha, torch.Tensor):
                    raise ValueError("alpha must be Tensor for multiclass")
                at = self.alpha.to(inputs.device).gather(0, targets.view(-1))
            else:
                at = 1.0
            loss = -at * (1 - pt) ** self.gamma * logpt

        # å¿½ç•¥ç‰¹å®šæ ‡ç­¾
        if self.ignore_index >= 0:
            valid = targets.view(-1) != self.ignore_index
            loss = loss[valid]

        # Reduction
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss
                 
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='ResGSCA Training')
    parser.add_argument('--data_dir', type=str, default='./database/3slice/48', help='Data directory')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')
    parser.add_argument('--epochs', type=int, default=150, help='Number of epochs')
    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')
    parser.add_argument('--save_dir', type=str, default='./resgsca_checkpoint/', help='Directory to save checkpoints')
    parser.add_argument('--device', type=str, default=None, help='Device to use (cuda or cpu)')
    parser.add_argument('--cv', action='store_true', help='Use cross-validation')
    parser.add_argument('--folds', type=int, default=10, help='Number of folds for cross-validation')
    parser.add_argument('--stratified', action='store_true', help='Use stratified sampling for cross-validation')
    parser.add_argument('--cv_save_dir', type=str, default='./cv_checkpoints', help='Directory to save cross-validation checkpoints')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    parser.add_argument('--visualize_attention', action='store_true', help='Generate attention visualizations')
    parser.add_argument('--vis_save_dir', type=str, default='./attention_visualizations', help='Directory to save attention visualizations')
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    # è®¾ç½®è®¾å¤‡
    if args.device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    if args.cv:
        os.makedirs(args.cv_save_dir, exist_ok=True)
    
    # æ•°æ®ç›®å½•å’Œæ ‡ç­¾æ˜ å°„
    root_dirs = {
        'cancer': os.path.join(args.data_dir, 'cancer'),
        'nocancer': os.path.join(args.data_dir, 'nocancer'),
    }
    labels_map = {'cancer': 1, 'nocancer': 0}
    
    # æ•°æ®å¢å¼º
    transform = transforms.Compose([
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),
    ])
    
    # åŠ è½½æ•°æ®é›†
    full_dataset = NpzPatchDataset(root_dirs, labels_map, transform)
    
    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    label_list = [label for _, label in full_dataset]
    print("æ ‡ç­¾åˆ†å¸ƒ:", Counter(label_list))
    
    # å†³å®šæ˜¯å¦ä½¿ç”¨äº¤å‰éªŒè¯
    if args.cv:
        print(f"\n{'='*20} å¼€å§‹ {args.folds} æŠ˜äº¤å‰éªŒè¯ {'='*20}")
        start_time = time.time()
        
        # å®šä¹‰ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨åˆ›å»ºå‡½æ•°
        def create_optimizer(params):
            return optim.Lookahead(optim.RAdam(params, lr=args.lr))
        
        def create_scheduler(optimizer):
            return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
        
        # æ‰§è¡Œäº¤å‰éªŒè¯
        # åˆ›å»ºå¯è§†åŒ–ä¿å­˜ç›®å½•
        if args.visualize_attention:
            vis_save_dir = os.path.join(args.cv_save_dir, 'attention_maps')
            os.makedirs(vis_save_dir, exist_ok=True)
        else:
            vis_save_dir = None

        cv_results = cross_validation(
            dataset=full_dataset,
            model_class=CTModel,
            num_folds=args.folds,
            epochs=args.epochs,
            batch_size=args.batch_size,
            criterion=FocalLoss(alpha=0.25, gamma=2.0),
            optimizer_fn=create_optimizer,
            scheduler_fn=create_scheduler,
            device=device,
            save_dir=args.cv_save_dir,
            stratified=args.stratified,
            visualize_attention=args.visualize_attention
        )
        
        # ä¿å­˜äº¤å‰éªŒè¯ç»“æœåˆ°CSV
        results_df = pd.DataFrame(cv_results['fold_results'])
        results_csv_path = os.path.join(args.cv_save_dir, 'cv_results.csv')
        results_df.to_csv(results_csv_path, index=False)
        print(f"äº¤å‰éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {results_csv_path}")
        
        # æ‰“å°æ€»è¿è¡Œæ—¶é—´
        elapsed_time = time.time() - start_time
        hours, remainder = divmod(elapsed_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        print(f"\näº¤å‰éªŒè¯æ€»æ—¶é—´: {int(hours)}å°æ—¶ {int(minutes)}åˆ†é’Ÿ {seconds:.2f}ç§’")
        
    else:
        # å¸¸è§„è®­ç»ƒæ¨¡å¼ï¼ˆä¸ä½¿ç”¨äº¤å‰éªŒè¯ï¼‰
        print("\nä½¿ç”¨å¸¸è§„è®­ç»ƒæ¨¡å¼ï¼ˆ80%è®­ç»ƒ/20%éªŒè¯ï¼‰")
        
        # åˆ†å‰²æ•°æ®é›†
        val_size = int(0.2 * len(full_dataset))
        train_size = len(full_dataset) - val_size
        
        generator = torch.Generator().manual_seed(args.seed)
        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)
        
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)
        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)
        
        # åˆ›å»ºæ¨¡å‹
        model = CTModel(num_classes=2).to(device)
        
        # åˆ›å»ºæŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        criterion = FocalLoss(alpha=0.25, gamma=2.0)
        optimizer = optim.Lookahead(optim.RAdam(model.parameters(), lr=args.lr))
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
        
        print(f"\n{'='*20} å¼€å§‹å¸¸è§„è®­ç»ƒ {'='*20}")
        start_time = time.time()
        
        # è®­ç»ƒæ¨¡å‹
        best_f1 = 0.0
        save_path = os.path.join(args.save_dir, 'res2gcsa_best.pth')
        
        progress_bar = tqdm(range(args.epochs), desc="è®­ç»ƒè¿›åº¦", unit="epoch")
        for epoch in progress_bar:
            train_loss, acc, prec, rec, f1 = train(model, train_loader, val_loader, criterion, optimizer, scheduler, device)
            
            # åŠ¨æ€æ›´æ–°è¿›åº¦æ¡æè¿°
            progress_bar.set_description(
                f"Epoch {epoch+1}/{args.epochs} | Loss: {train_loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f}"
            )
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if f1 > best_f1:
                best_f1 = f1
                torch.save(model.state_dict(), save_path)
                print(f"  ğŸ‰ æ–°çš„æœ€ä½³F1: {best_f1:.4f}, å·²ä¿å­˜åˆ° {save_path}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_save_path = os.path.join(args.save_dir, 'res2gcsa_final.pth')
        torch.save(model.state_dict(), final_save_path)
        print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_save_path}")
        
        # æ‰“å°æ€»è¿è¡Œæ—¶é—´
        elapsed_time = time.time() - start_time
        hours, remainder = divmod(elapsed_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        print(f"\nè®­ç»ƒæ€»æ—¶é—´: {int(hours)}å°æ—¶ {int(minutes)}åˆ†é’Ÿ {seconds:.2f}ç§’")