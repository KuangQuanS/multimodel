{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c5c2c03",
   "metadata": {},
   "source": [
    "#### cfDNA存放目录：/home/maweicheng/database/cfDNA\n",
    "#### 癌症CT存放目录：/home/maweicheng/database/khct/patch_output/split_libs\n",
    "#### 正常结节存放目录：/home/maweicheng/resgsca/database/3slice/64/nocancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfda5711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "癌症样本数: 40\n",
      "正常样本数: 340\n",
      "已保存： ../database/cfDNA/train.npz ../database/cfDNA/test.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "modalities = [\"Frag\", \"CNV\", \"PFE\", \"NDR\", \"NDR2K\"]\n",
    "cf_base_dir = \"../database/cfDNA\"\n",
    "ct_cancer_dir = \"/home/maweicheng/database/khct/patch_output/split_libs\"\n",
    "ct_normal_dir = \"/home/maweicheng/resgsca/database/3slice/64/nocancer\"\n",
    "\n",
    "save_train_path = os.path.join(cf_base_dir, \"train.npz\")\n",
    "save_test_path = os.path.join(cf_base_dir, \"test.npz\")\n",
    "\n",
    "# 1. 读取所有模态数据\n",
    "cf_data = {}\n",
    "for mod in modalities:\n",
    "    df_norm = pd.read_csv(os.path.join(cf_base_dir, \"normal\", f\"healthy_{mod}.csv\"), index_col=0)\n",
    "    df_can  = pd.read_csv(os.path.join(cf_base_dir, \"cancer\",  f\"cancer_{mod}.csv\"), index_col=0)\n",
    "    df_norm = df_norm.fillna(df_norm.mean())\n",
    "    df_can  = df_can.fillna(df_can.mean())\n",
    "    cf_data[mod] = {\"normal\": df_norm, \"cancer\": df_can}\n",
    "\n",
    "# 获取共同的 index（癌症部分）\n",
    "to_remove = ['Lib-006', 'Lib-020', 'Lib-022', 'Lib-024', 'Lib-027', 'Lib-031', 'Lib-032', 'Lib-024', 'Lib-034', 'Lib-036', 'Lib-038', 'Lib-040'\n",
    "             , 'Lib-042', 'Lib-044', 'Lib-046', 'Lib-048', 'Lib-050', 'Lib-050', 'Lib-051', 'Lib-054', 'Lib-056', 'Lib-057', 'Lib-058', 'Lib-063'\n",
    "             , 'Lib-065', 'Lib-067', 'Lib-069', 'Lib-071', 'Lib-073', 'Lib-075', 'Lib-077', 'Lib-079', 'Lib-081', 'Lib-082', 'Lib-090', 'Lib-091'\n",
    "             , 'Lib-092', 'Lib-093', 'Lib-094', 'Lib-095', 'Lib-096', 'Lib-097', 'Lib-110', 'Lib-121', 'Lib-122', 'Lib-123', 'Lib-137', 'Lib-138'\n",
    "             , 'Lib-139', 'Lib-140', 'Lib-141', 'Lib-142', 'Lib-143', 'Lib-144', 'Lib-149', 'Lib-150', 'Lib-151', 'Lib-152', 'Lib-154', 'Lib-156'\n",
    "             , 'Lib-158', 'Lib-160'\n",
    "             ]\n",
    "cancer_ids = cf_data[\"Frag\"][\"cancer\"].index[~cf_data[\"Frag\"][\"cancer\"].index.isin(to_remove)]\n",
    "normal_ids = cf_data[\"Frag\"][\"normal\"].index.tolist()\n",
    "\n",
    "# 2. 处理 cancer 样本\n",
    "X_cancer = {mod: [] for mod in modalities}\n",
    "CT_cancer = []\n",
    "y_cancer = []\n",
    "\n",
    "\n",
    "for cid in cancer_ids:\n",
    "    ct_pattern = os.path.join(ct_cancer_dir, cid, \"*.npz\")\n",
    "    ct_files = glob.glob(ct_pattern)\n",
    "\n",
    "    # 确保该病人目录下确实有一个 .npz 文件\n",
    "    if len(ct_files) != 1:\n",
    "        continue  # 无或多个 npz 文件都跳过\n",
    "\n",
    "    ct_path = ct_files[0]\n",
    "    try:\n",
    "        ct_npz = np.load(ct_path)\n",
    "        ct_data = ct_npz[\"data\"]\n",
    "        if ct_data.shape == (3, 64, 64):\n",
    "            ct_data = np.transpose(ct_data, (1, 2, 0)) \n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Failed to load CT for {cid}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 检查 cfDNA 模态是否齐全\n",
    "    valid = all(cid in cf_data[mod][\"cancer\"].index for mod in modalities)\n",
    "    if not valid:\n",
    "        continue\n",
    "\n",
    "    # 添加数据\n",
    "    for mod in modalities:\n",
    "        X_cancer[mod].append(cf_data[mod][\"cancer\"].loc[cid].values.astype(np.float32))\n",
    "    CT_cancer.append(ct_data.astype(np.float32))\n",
    "    y_cancer.append(1)\n",
    "\n",
    "# 3. 处理 normal 样本\n",
    "X_normal = {mod: [] for mod in modalities}\n",
    "CT_normal = []\n",
    "y_normal = []\n",
    "\n",
    "used_nocancer = set()\n",
    "available_ct_files = sorted(os.listdir(ct_normal_dir))\n",
    "\n",
    "for nid in normal_ids:\n",
    "    # 随机选择未使用的 CT 文件\n",
    "    candidates = list(set(available_ct_files) - used_nocancer)\n",
    "    if not candidates:\n",
    "        break  # 不够用了\n",
    "    selected = random.choice(candidates)\n",
    "    used_nocancer.add(selected)\n",
    "    ct_path = os.path.join(ct_normal_dir, selected)\n",
    "    try:\n",
    "        ct_npz = np.load(ct_path)\n",
    "        ct_data = ct_npz[\"data\"]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    for mod in modalities:\n",
    "        X_normal[mod].append(cf_data[mod][\"normal\"].loc[nid].values.astype(np.float32))\n",
    "    CT_normal.append(ct_data.astype(np.float32))\n",
    "    y_normal.append(0)\n",
    "\n",
    "# 4. 组装训练与测试集（90% 训练 + 10% 测试，按顺序切分）\n",
    "def split_data(X_dict, CT_list, y_list):\n",
    "    n = len(y_list)\n",
    "    split = int(n * 0.1)\n",
    "    idx_test, idx_train = np.arange(split), np.arange(split, n)\n",
    "\n",
    "    def subset(data, idx):\n",
    "        return [np.array(data[i]) for i in idx]\n",
    "\n",
    "    train = {mod: np.stack(subset(X_dict[mod], idx_train)) for mod in modalities}\n",
    "    test  = {mod: np.stack(subset(X_dict[mod], idx_test))  for mod in modalities}\n",
    "    train[\"CT\"] = np.stack(subset(CT_list, idx_train))\n",
    "    test[\"CT\"]  = np.stack(subset(CT_list, idx_test))\n",
    "    train[\"y\"]  = np.array([y_list[i] for i in idx_train])\n",
    "    test[\"y\"]   = np.array([y_list[i] for i in idx_test])\n",
    "    return train, test\n",
    "\n",
    "print(f\"癌症样本数: {len(y_cancer)}\")\n",
    "print(f\"正常样本数: {len(y_normal)}\")\n",
    "\n",
    "train_can, test_can = split_data(X_cancer, CT_cancer, y_cancer)\n",
    "train_nor, test_nor = split_data(X_normal, CT_normal, y_normal)\n",
    "\n",
    "# 合并\n",
    "train_all = {key: np.concatenate([train_can[key], train_nor[key]]) for key in train_can}\n",
    "test_all  = {key: np.concatenate([test_can[key], test_nor[key]])   for key in test_can}\n",
    "\n",
    "# 5. 保存\n",
    "np.savez_compressed(save_train_path, **train_all)\n",
    "np.savez_compressed(save_test_path,  **test_all)\n",
    "print(\"已保存：\", save_train_path, save_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e08aba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "癌症样本数: 79\n",
      "正常样本数: 340\n",
      "已保存： ../database/cfDNA/train.npz ../database/cfDNA/test.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "modalities = [\"Frag\", \"CNV\", \"PFE\", \"NDR\", \"NDR2K\"]\n",
    "cf_base_dir = \"../database/cfDNA\"\n",
    "ct_cancer_dir = \"/home/maweicheng/database/khct/patch_output/split_libs\"\n",
    "ct_normal_dir = \"/home/maweicheng/resgsca/database/3slice/64/nocancer\"\n",
    "\n",
    "save_train_path = os.path.join(cf_base_dir, \"train.npz\")\n",
    "save_test_path = os.path.join(cf_base_dir, \"test.npz\")\n",
    "\n",
    "# 1. 读取所有模态数据\n",
    "cf_data = {}\n",
    "for mod in modalities:\n",
    "    df_norm = pd.read_csv(os.path.join(cf_base_dir, \"normal\", f\"healthy_{mod}.csv\"), index_col=0)\n",
    "    df_can  = pd.read_csv(os.path.join(cf_base_dir, \"cancer\",  f\"cancer_{mod}.csv\"), index_col=0)\n",
    "    df_norm = df_norm.fillna(df_norm.mean())\n",
    "    df_can  = df_can.fillna(df_can.mean())\n",
    "    cf_data[mod] = {\"normal\": df_norm, \"cancer\": df_can}\n",
    "\n",
    "# 获取共同的 index（癌症部分）\n",
    "cancer_ids = cf_data[\"Frag\"][\"cancer\"].index.tolist()\n",
    "normal_ids = cf_data[\"Frag\"][\"normal\"].index.tolist()\n",
    "\n",
    "\n",
    "# 2. 处理 cancer 样本\n",
    "X_cancer = {mod: [] for mod in modalities}\n",
    "CT_cancer = []\n",
    "y_cancer = []\n",
    "\n",
    "\n",
    "for cid in cancer_ids:\n",
    "    ct_pattern = os.path.join(ct_cancer_dir, cid, \"*.npz\")\n",
    "    ct_files = glob.glob(ct_pattern)\n",
    "\n",
    "    # 确保该病人目录下确实有一个 .npz 文件\n",
    "    if len(ct_files) != 1:\n",
    "        continue  # 无或多个 npz 文件都跳过\n",
    "\n",
    "    ct_path = ct_files[0]\n",
    "    try:\n",
    "        ct_npz = np.load(ct_path)\n",
    "        ct_data = ct_npz[\"data\"]\n",
    "        if ct_data.shape == (3, 64, 64):\n",
    "            ct_data = np.transpose(ct_data, (1, 2, 0)) \n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Failed to load CT for {cid}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 检查 cfDNA 模态是否齐全\n",
    "    valid = all(cid in cf_data[mod][\"cancer\"].index for mod in modalities)\n",
    "    if not valid:\n",
    "        continue\n",
    "\n",
    "    # 添加数据\n",
    "    for mod in modalities:\n",
    "        X_cancer[mod].append(cf_data[mod][\"cancer\"].loc[cid].values.astype(np.float32))\n",
    "    CT_cancer.append(ct_data.astype(np.float32))\n",
    "    y_cancer.append(1)\n",
    "\n",
    "# 3. 处理 normal 样本\n",
    "X_normal = {mod: [] for mod in modalities}\n",
    "CT_normal = []\n",
    "y_normal = []\n",
    "\n",
    "used_nocancer = set()\n",
    "available_ct_files = sorted(os.listdir(ct_normal_dir))\n",
    "\n",
    "for nid in normal_ids:\n",
    "    # 随机选择未使用的 CT 文件\n",
    "    candidates = list(set(available_ct_files) - used_nocancer)\n",
    "    if not candidates:\n",
    "        break  # 不够用了\n",
    "    selected = random.choice(candidates)\n",
    "    used_nocancer.add(selected)\n",
    "    ct_path = os.path.join(ct_normal_dir, selected)\n",
    "    try:\n",
    "        ct_npz = np.load(ct_path)\n",
    "        ct_data = ct_npz[\"data\"]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    for mod in modalities:\n",
    "        X_normal[mod].append(cf_data[mod][\"normal\"].loc[nid].values.astype(np.float32))\n",
    "    CT_normal.append(ct_data.astype(np.float32))\n",
    "    y_normal.append(0)\n",
    "\n",
    "# 4. 组装训练与测试集（90% 训练 + 10% 测试，按顺序切分）\n",
    "def split_data(X_dict, CT_list, y_list):\n",
    "    n = len(y_list)\n",
    "    split = int(n * 0.1)\n",
    "    idx_test, idx_train = np.arange(split), np.arange(split, n)\n",
    "\n",
    "    def subset(data, idx):\n",
    "        return [np.array(data[i]) for i in idx]\n",
    "\n",
    "    train = {mod: np.stack(subset(X_dict[mod], idx_train)) for mod in modalities}\n",
    "    test  = {mod: np.stack(subset(X_dict[mod], idx_test))  for mod in modalities}\n",
    "    train[\"CT\"] = np.stack(subset(CT_list, idx_train))\n",
    "    test[\"CT\"]  = np.stack(subset(CT_list, idx_test))\n",
    "    train[\"y\"]  = np.array([y_list[i] for i in idx_train])\n",
    "    test[\"y\"]   = np.array([y_list[i] for i in idx_test])\n",
    "    return train, test\n",
    "\n",
    "print(f\"癌症样本数: {len(y_cancer)}\")\n",
    "print(f\"正常样本数: {len(y_normal)}\")\n",
    "\n",
    "train_can, test_can = split_data(X_cancer, CT_cancer, y_cancer)\n",
    "train_nor, test_nor = split_data(X_normal, CT_normal, y_normal)\n",
    "\n",
    "# 合并\n",
    "train_all = {key: np.concatenate([train_can[key], train_nor[key]]) for key in train_can}\n",
    "test_all  = {key: np.concatenate([test_can[key], test_nor[key]])   for key in test_can}\n",
    "\n",
    "# 5. 保存\n",
    "np.savez_compressed(save_train_path, **train_all)\n",
    "np.savez_compressed(save_test_path,  **test_all)\n",
    "print(\"已保存：\", save_train_path, save_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20041f0a",
   "metadata": {},
   "source": [
    "# 下面这个加了id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24028959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "modalities = [\"Frag\", \"CNV\", \"PFE\", \"NDR\", \"NDR2K\"]\n",
    "cf_base_dir = \"../database/cfDNA\"\n",
    "ct_cancer_dir = \"/home/maweicheng/database/khct/patch_output/split_libs\"\n",
    "ct_normal_dir = \"/home/maweicheng/resgsca/database/3slice/64/nocancer\"\n",
    "\n",
    "save_train_path = os.path.join(cf_base_dir, \"train.npz\")\n",
    "save_test_path = os.path.join(cf_base_dir, \"test.npz\")\n",
    "\n",
    "# 1. 读取所有模态数据\n",
    "cf_data = {}\n",
    "for mod in modalities:\n",
    "    df_norm = pd.read_csv(os.path.join(cf_base_dir, \"normal\", f\"healthy_{mod}.csv\"), index_col=0)\n",
    "    df_can  = pd.read_csv(os.path.join(cf_base_dir, \"cancer\",  f\"cancer_{mod}.csv\"), index_col=0)\n",
    "    df_norm = df_norm.fillna(df_norm.mean())\n",
    "    df_can  = df_can.fillna(df_can.mean())\n",
    "    cf_data[mod] = {\"normal\": df_norm, \"cancer\": df_can}\n",
    "\n",
    "# 获取 index\n",
    "cancer_ids = cf_data[\"Frag\"][\"cancer\"].index.tolist()\n",
    "normal_ids = cf_data[\"Frag\"][\"normal\"].index.tolist()\n",
    "\n",
    "# 2. 处理 cancer 样本\n",
    "X_cancer = {mod: [] for mod in modalities}\n",
    "CT_cancer = []\n",
    "y_cancer = []\n",
    "id_cancer = []\n",
    "\n",
    "for cid in cancer_ids:\n",
    "    ct_pattern = os.path.join(ct_cancer_dir, cid, \"*.npz\")\n",
    "    ct_files = glob.glob(ct_pattern)\n",
    "\n",
    "    if len(ct_files) != 1:\n",
    "        continue\n",
    "\n",
    "    ct_path = ct_files[0]\n",
    "    try:\n",
    "        ct_npz = np.load(ct_path)\n",
    "        ct_data = ct_npz[\"data\"]\n",
    "        if ct_data.shape == (3, 64, 64):\n",
    "            ct_data = np.transpose(ct_data, (1, 2, 0))\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Failed to load CT for {cid}: {e}\")\n",
    "        continue\n",
    "\n",
    "    valid = all(cid in cf_data[mod][\"cancer\"].index for mod in modalities)\n",
    "    if not valid:\n",
    "        continue\n",
    "\n",
    "    for mod in modalities:\n",
    "        X_cancer[mod].append(cf_data[mod][\"cancer\"].loc[cid].values.astype(np.float32))\n",
    "    CT_cancer.append(ct_data.astype(np.float32))\n",
    "    y_cancer.append(1)\n",
    "    id_cancer.append(cid)\n",
    "\n",
    "# 3. 处理 normal 样本\n",
    "X_normal = {mod: [] for mod in modalities}\n",
    "CT_normal = []\n",
    "y_normal = []\n",
    "id_normal = []\n",
    "\n",
    "used_nocancer = set()\n",
    "available_ct_files = sorted(os.listdir(ct_normal_dir))\n",
    "\n",
    "for nid in normal_ids:\n",
    "    candidates = list(set(available_ct_files) - used_nocancer)\n",
    "    if not candidates:\n",
    "        break\n",
    "    selected = random.choice(candidates)\n",
    "    used_nocancer.add(selected)\n",
    "    ct_path = os.path.join(ct_normal_dir, selected)\n",
    "    try:\n",
    "        ct_npz = np.load(ct_path)\n",
    "        ct_data = ct_npz[\"data\"]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    for mod in modalities:\n",
    "        X_normal[mod].append(cf_data[mod][\"normal\"].loc[nid].values.astype(np.float32))\n",
    "    CT_normal.append(ct_data.astype(np.float32))\n",
    "    y_normal.append(0)\n",
    "    id_normal.append(os.path.splitext(selected)[0])  # 去掉 .npz 后缀\n",
    "\n",
    "# 4. 组装训练与测试集（90% 训练 + 10% 测试）\n",
    "def split_data(X_dict, CT_list, y_list, id_list):\n",
    "    n = len(y_list)\n",
    "    split = int(n * 0.1)\n",
    "    idx_test, idx_train = np.arange(split), np.arange(split, n)\n",
    "\n",
    "    def subset(data, idx):\n",
    "        return [data[i] for i in idx]\n",
    "\n",
    "    train = {mod: np.stack(subset(X_dict[mod], idx_train)) for mod in modalities}\n",
    "    test  = {mod: np.stack(subset(X_dict[mod], idx_test))  for mod in modalities}\n",
    "    train[\"CT\"] = np.stack(subset(CT_list, idx_train))\n",
    "    test[\"CT\"]  = np.stack(subset(CT_list, idx_test))\n",
    "    train[\"y\"]  = np.array([y_list[i] for i in idx_train])\n",
    "    test[\"y\"]   = np.array([y_list[i] for i in idx_test])\n",
    "    train[\"id\"] = np.array([id_list[i] for i in idx_train])\n",
    "    test[\"id\"]  = np.array([id_list[i] for i in idx_test])\n",
    "    return train, test\n",
    "\n",
    "print(f\"癌症样本数: {len(y_cancer)}\")\n",
    "print(f\"正常样本数: {len(y_normal)}\")\n",
    "\n",
    "train_can, test_can = split_data(X_cancer, CT_cancer, y_cancer, id_cancer)\n",
    "train_nor, test_nor = split_data(X_normal, CT_normal, y_normal, id_normal)\n",
    "\n",
    "# 合并\n",
    "train_all = {key: np.concatenate([train_can[key], train_nor[key]]) for key in train_can}\n",
    "test_all  = {key: np.concatenate([test_can[key], test_nor[key]])   for key in test_can}\n",
    "\n",
    "# 5. 保存\n",
    "np.savez_compressed(save_train_path, **train_all)\n",
    "np.savez_compressed(save_test_path,  **test_all)\n",
    "print(\"已保存：\", save_train_path, save_test_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
