"""
ç®€åŒ–çš„å¤šæ¨¡æ€åˆ†ç±»è®­ç»ƒè„šæœ¬
"""

import os
import sys
import argparse
import logging
import json
from datetime import datetime
import random
import numpy as np
import torch
from torch.utils.data import DataLoader
from sklearn.model_selection import StratifiedKFold, train_test_split
from tabulate import tabulate
from tqdm import tqdm

from fusion_models import CrossAttentionFusion
from dataset import load_data, create_feature_selectors, MultiModalDataset
from training_utils import (
    load_encoder, create_ct_model, run_single_fold, 
    evaluate, plot_roc_curve, plot_training_curves, plot_all_folds_curves
)
from evaluation import evaluate_on_test_set, evaluate_single_training

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    # è®¾ç½®cudnnä»¥ç¡®ä¿ç¡®å®šæ€§è¡Œä¸º
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    logger.info(f"éšæœºç§å­è®¾ç½®ä¸º: {seed}")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Multi-modal Classification Training')
    
    # Data parameters
    parser.add_argument('--data_file', type=str, required=True, help='Path to training data NPZ file')
    parser.add_argument('--test_file', type=str, help='Path to test data NPZ file')
    parser.add_argument('--modalities', nargs='+', default=['Frag', 'PFE', 'NDR', 'NDR2K'], 
                       help='List of modalities to use')
    
    # Feature selection parameters
    parser.add_argument('--use_feature_selection', action='store_true', help='Enable cfDNA feature selection')
    parser.add_argument('--feature_selection_method', type=str, default='combined',
                       choices=['variance', 'kbest', 'rfe', 'combined', 'lasso_voting', 'elastic_voting', 'pca'],
                       help='Feature selection method')
    parser.add_argument('--k_features', type=int, default=50, help='Number of features to select per cfDNA modality')
    parser.add_argument('--lasso_voting_threshold', type=float, default=0.6, help='Voting threshold for LASSO feature selection')
    parser.add_argument('--lasso_n_runs', type=int, default=10, help='Number of LASSO runs for voting')
    parser.add_argument('--elastic_l1_ratio', type=float, default=0.7, help='ElasticNet L1 ratio')
    parser.add_argument('--variance_threshold', type=float, default=0.0001, help='Variance threshold for initial filtering')
    
    # Model parameters
    parser.add_argument('--latent_dim', type=int, default=256, help='Latent dimension for encoders')
    parser.add_argument('--no_ct', action='store_true', help='Disable CT modality')
    parser.add_argument('--ct_model_path', type=str, default='CT/resgsca_checkpoint/best_model.pth', 
                       help='Path to pre-trained CT model')
    parser.add_argument('--finetune_ct', action='store_true', help='Fine-tune CT feature extractor')
    parser.add_argument('--no_se_block', action='store_true', help='Disable SE block in model')
    parser.add_argument('--finetune', action='store_true', help='Fine-tune encoders')
    parser.add_argument('--l1_lambda', type=float, default=0.0, help='L1 regularization strength')
    
    # Training parameters
    parser.add_argument('--epochs', type=int, default=100, help='Number of training epochs')
    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')
    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=1e-5, help='Weight decay')
    parser.add_argument('--patience', type=int, default=4, help='Early stopping patience')
    parser.add_argument('--strong_augment', action='store_true', help='Use strong data augmentation')
    parser.add_argument('--aug_prob', type=float, default=0.8, help='Probability of applying augmentation')
    
    # Cross-validation parameters
    parser.add_argument('--k_folds', type=int, default=5, help='Number of folds for cross-validation')
    parser.add_argument('--split_ratio', type=float, default=0.8, help='Train/validation split ratio for single training')
    
    # Output parameters
    parser.add_argument('--output_dir', type=str, default='results', help='Output directory for results')
    parser.add_argument('--use_test_set', action='store_true', help='Evaluate on independent test set after training')
    parser.add_argument('--test_eval_strategy', type=str, default='both', 
                       choices=['best', 'ensemble', 'both'], help='Test evaluation strategy')
    
    # Reproducibility parameters
    parser.add_argument('--seed', type=int, default=42, help='Random seed for reproducibility')
    
    # Class imbalance handling
    parser.add_argument('--auto_class_weights', action='store_true', default=True, 
                       help='Automatically calculate class weights based on class distribution (default: True)')
    parser.add_argument('--no_auto_class_weights', dest='auto_class_weights', action='store_false',
                       help='Disable automatic class weight calculation')
    
    # Cross-fold error analysis
    parser.add_argument('--analyze_errors', action='store_true', 
                       help='Analyze samples that are consistently misclassified across folds')
    parser.add_argument('--min_error_folds', type=int, default=2, 
                       help='Minimum number of folds where sample must be wrong to be flagged')
    
    args = parser.parse_args()
    args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    return args


def cross_validate(args, dataset, feature_selectors):
    """è¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ"""
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    cv_dir = os.path.join(args.output_dir, f'cv_results_{timestamp}')
    os.makedirs(cv_dir, exist_ok=True)
    
    logger.info(f"å¼€å§‹{args.k_folds}æŠ˜äº¤å‰éªŒè¯")
    logger.info(f"ç»“æœä¿å­˜åˆ°: {cv_dir}")
    
    # è·å–ç‰¹å¾ç»´åº¦
    feature_dims = {}
    sample = dataset[0]
    for i, mod in enumerate(args.modalities):
        key = f'X{i}'
        if key in sample:
            feature_dims[mod] = sample[key].shape[0]
            logger.info(f"{mod}ç‰¹å¾ç»´åº¦: {feature_dims[mod]}")
    
    # äº¤å‰éªŒè¯è®¾ç½®
    kf = StratifiedKFold(n_splits=args.k_folds, shuffle=True, random_state=42)
    all_results = []
    
    for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset)), dataset.y), 1):
        logger.info(f"\n{'='*15} Fold {fold}/{args.k_folds} {'='*15}")
        
        # é‡æ–°åˆå§‹åŒ–ç¼–ç å™¨å‚æ•°ï¼Œé¿å…foldé—´å‚æ•°æ³„éœ²
        logger.debug("é‡æ–°åˆå§‹åŒ–ç¼–ç å™¨å‚æ•°")
        encoders = {mod: load_encoder(mod, None, args.latent_dim, feature_dims).to(args.device) 
                   for mod in args.modalities}

        for encoder in encoders.values():
            if args.finetune:
                encoder.train()
            else:
                encoder.eval()
                for p in encoder.parameters():
                    p.requires_grad = False

        # æ¯ä¸ªfoldéƒ½é‡æ–°åˆ›å»ºCTæ¨¡å‹
        ct_model = None if args.no_ct else create_ct_model(args.ct_model_path, args.device)
        
        # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
        train_dataset_fold = MultiModalDataset(
            args.data_file, args.modalities,
            indices=train_idx,
            include_ct=not args.no_ct,
            feature_selectors=feature_selectors,
            training=True,  # å¯ç”¨æ•°æ®å¢å¼º
            strong_augment=args.strong_augment  # å¼ºå¢å¼ºé€‰é¡¹
        )
        
        val_dataset_fold = MultiModalDataset(
            args.data_file, args.modalities,
            indices=val_idx,
            include_ct=not args.no_ct,
            feature_selectors=feature_selectors,
            training=False,  # ç¦ç”¨æ•°æ®å¢å¼º
            strong_augment=False  # éªŒè¯é›†ä¸ä½¿ç”¨å¢å¼º
        )
        
        train_loader = DataLoader(train_dataset_fold, batch_size=args.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset_fold, batch_size=args.batch_size, shuffle=False)
        
        # æ¯ä¸ªfoldéƒ½é‡æ–°åˆ›å»ºä¸»æ¨¡å‹
        model = CrossAttentionFusion(
            dim_latent=args.latent_dim,
            n_modalities=len(args.modalities),
            num_classes=2,
            ct_feature_extractor=ct_model,
            finetune_ct=args.finetune_ct,
            use_ct=not args.no_ct,
            use_se_block=not args.no_se_block
        ).to(args.device)
        
        fold_results, best_epoch = run_single_fold(
            model, encoders, train_loader, val_loader, args, fold, cv_dir
        )
        
        if fold_results is None:
            logger.warning(f"Fold {fold} è®­ç»ƒå¤±è´¥")
            continue
            
        fold_results['fold'] = fold
        fold_results['best_epoch'] = best_epoch
        all_results.append(fold_results)
        
        # è·å–æœ€ç»ˆçš„è®­ç»ƒå’ŒéªŒè¯æŸå¤±ï¼ˆä»training_historyçš„æœ€åä¸€ä¸ªå€¼ï¼‰
        if 'training_history' in fold_results:
            final_train_loss = fold_results['training_history']['train_losses'][-1] if fold_results['training_history']['train_losses'] else 0.0
            # è·å–æœ€ä½³epochçš„éªŒè¯æŸå¤±ï¼Œè€Œä¸æ˜¯æœ€åä¸€ä¸ªepochçš„
            final_val_loss = fold_results.get('loss', 0.0)  # è¿™æ˜¯æœ€ä½³epochçš„éªŒè¯æŸå¤±
        else:
            final_train_loss = 0.0
            final_val_loss = fold_results.get('loss', 0.0)
        
        logger.info(f"Fold {fold} å®Œæˆ - Acc: {fold_results['accuracy']:.4f}, AUC: {fold_results['auc']:.4f}, F1: {fold_results['f1']:.4f}, Train_Loss: {final_train_loss:.4f}, Val_Loss: {final_val_loss:.4f}")
        
        # ä¿å­˜foldè¯¦ç»†ç»“æœï¼ˆåŒ…å«é”™è¯¯æ ·æœ¬ä¿¡æ¯ï¼‰
        fold_detail_file = os.path.join(cv_dir, f'fold_{fold}_results.json')
        fold_detail = {
            'fold_number': fold,
            'accuracy': fold_results['accuracy'],
            'f1': fold_results['f1'],
            'auc': fold_results['auc'],
            'loss': fold_results.get('loss', 0.0),
            'errors': fold_results.get('errors', []),
            'num_errors': len(fold_results.get('errors', [])),
            'total_samples': len(fold_results.get('labels', [])),
            'error_rate': len(fold_results.get('errors', [])) / len(fold_results.get('labels', [])) if fold_results.get('labels') else 0.0
        }
        
        try:
            with open(fold_detail_file, 'w', encoding='utf-8') as f:
                json.dump(fold_detail, f, indent=2, ensure_ascii=False)
            logger.debug(f"Fold {fold} è¯¦ç»†ç»“æœå·²ä¿å­˜: {len(fold_detail['errors'])} ä¸ªé”™è¯¯æ ·æœ¬")
        except Exception as e:
            logger.warning(f"ä¿å­˜fold {fold} è¯¦ç»†ç»“æœå¤±è´¥: {e}")
        
        # ç»˜åˆ¶å•foldç»“æœ
        plot_roc_curve(fold_results['labels'], fold_results['probs'], cv_dir, f"fold_{fold}")
        plot_training_curves(fold_results, cv_dir, fold, show=False)
    
    # å¤„ç†äº¤å‰éªŒè¯ç»“æœ
    if not all_results:
        logger.error("æ‰€æœ‰foldéƒ½å¤±è´¥äº†")
        return None
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    auc_scores = [r['auc'] for r in all_results]
    acc_scores = [r['accuracy'] for r in all_results] 
    f1_scores = [r['f1'] for r in all_results]
    
    results_dict = {
        'results': all_results,
        'mean_auc': np.mean(auc_scores),
        'std_auc': np.std(auc_scores),
        'mean_accuracy': np.mean(acc_scores),
        'std_accuracy': np.std(acc_scores),
        'mean_f1': np.mean(f1_scores),
        'std_f1': np.std(f1_scores),
        'best_fold': all_results[np.argmax(auc_scores)]['fold'],
        'cv_dir': cv_dir
    }
    
    # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
    np.savez(
        os.path.join(cv_dir, 'cv_results.npz'),
        **{k: v for k, v in results_dict.items() if k != 'cv_dir'}
    )
    
    # è¾“å‡ºç»“æœè¡¨æ ¼
    table_data = []
    for result in all_results:
        table_data.append([
            result['fold'],
            f"{result['accuracy']:.4f}",
            f"{result['f1']:.4f}",
            f"{result['auc']:.4f}",
            result['best_epoch']
        ])
    
    table_data.append([
        "å¹³å‡å€¼",
        f"{results_dict['mean_accuracy']:.4f} Â± {results_dict['std_accuracy']:.4f}",
        f"{results_dict['mean_f1']:.4f} Â± {results_dict['std_f1']:.4f}", 
        f"{results_dict['mean_auc']:.4f} Â± {results_dict['std_auc']:.4f}", 
        "-"
    ])
    
    logger.info(f"\näº¤å‰éªŒè¯ç»“æœ:")
    print(tabulate(table_data, headers=["Fold", "Accuracy", "F1 Score", "AUC Score", "Best Epoch"], tablefmt="grid"))
    
    logger.info(f"äº¤å‰éªŒè¯å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {cv_dir}")
    
    # ç»˜åˆ¶æ‰€æœ‰æŠ˜çš„è®­ç»ƒæ›²çº¿å¯¹æ¯”å›¾
    plot_all_folds_curves(all_results, cv_dir)
    
    # è·¨foldé”™è¯¯æ ·æœ¬åˆ†æ
    if args.analyze_errors:
        logger.info("å¼€å§‹è·¨foldé”™è¯¯æ ·æœ¬åˆ†æ...")
        analyze_cross_fold_errors(args, dataset, feature_selectors, cv_dir, args.min_error_folds)
    
    # ç‹¬ç«‹æµ‹è¯•é›†è¯„ä¼°
    if args.test_file and args.use_test_set:
        logger.info("å¼€å§‹ç‹¬ç«‹æµ‹è¯•é›†è¯„ä¼°...")
        evaluate_on_test_set(args, feature_selectors, results_dict)
        
    return results_dict


def run_single_training(args, dataset, feature_selectors=None):
    """è¿è¡Œå•æ¬¡è®­ç»ƒï¼ˆä¸ä½¿ç”¨äº¤å‰éªŒè¯ï¼‰"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = os.path.join(args.output_dir, f'single_training_{timestamp}')
    os.makedirs(output_dir, exist_ok=True)
    
    logger.info(f"å¼€å§‹å•æ¬¡è®­ç»ƒ")
    logger.info(f"ç»“æœä¿å­˜åˆ°: {output_dir}")
    
    # è·å–ç‰¹å¾ç»´åº¦
    feature_dims = {}
    sample = dataset[0]
    for i, mod in enumerate(args.modalities):
        key = f'X{i}'
        if key in sample:
            feature_dims[mod] = sample[key].shape[0]
    
    # åˆ’åˆ†è®­ç»ƒéªŒè¯é›†
    train_indices, val_indices = train_test_split(
        np.arange(len(dataset)), 
        test_size=1-args.split_ratio, 
        stratify=dataset.y, 
        random_state=42
    )
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = MultiModalDataset(
        args.data_file, args.modalities,
        indices=train_indices,
        include_ct=not args.no_ct,
        feature_selectors=feature_selectors,
        training=True,
        strong_augment=args.strong_augment
    )
    
    val_dataset = MultiModalDataset(
        args.data_file, args.modalities,
        indices=val_indices,
        include_ct=not args.no_ct,
        feature_selectors=feature_selectors,
        training=False,
        strong_augment=False
    )
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    encoders = {mod: load_encoder(mod, None, args.latent_dim, feature_dims).to(args.device) 
               for mod in args.modalities}
    
    for encoder in encoders.values():
        if args.finetune:
            encoder.train()
        else:
            encoder.eval()
            for p in encoder.parameters():
                p.requires_grad = False
    
    ct_model = None if args.no_ct else create_ct_model(args.ct_model_path, args.device)
    
    model = CrossAttentionFusion(
        dim_latent=args.latent_dim,
        n_modalities=len(args.modalities),
        num_classes=2,
        ct_feature_extractor=ct_model,
        finetune_ct=args.finetune_ct,
        use_ct=not args.no_ct,
        use_se_block=not args.no_se_block
    ).to(args.device)
    
    # è®­ç»ƒ
    results, best_epoch = run_single_fold(
        model, encoders, train_loader, val_loader, args, 1, output_dir
    )
    
    if results:
        logger.info(f"è®­ç»ƒå®Œæˆ - AUC: {results['auc']:.4f}, F1: {results['f1']:.4f}")
        plot_roc_curve(results['labels'], results['probs'], output_dir)
        plot_training_curves(results, output_dir, show=False)
        
        # ç‹¬ç«‹æµ‹è¯•é›†è¯„ä¼°
        if args.test_file:
            logger.info("å¼€å§‹ç‹¬ç«‹æµ‹è¯•é›†è¯„ä¼°...")
            evaluate_single_training(args, results, output_dir, feature_selectors)
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
    set_seed(args.seed)
    
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {args.device}")
    logger.info(f"ä½¿ç”¨æ¨¡æ€: {args.modalities}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆ›å»ºç‰¹å¾é€‰æ‹©å™¨
    feature_selectors = None
    if args.use_feature_selection:
        logger.info("åˆ›å»ºcfDNAç‰¹å¾é€‰æ‹©å™¨...")
        feature_selectors = create_feature_selectors(
            args.data_file, 
            args.modalities, 
            method=args.feature_selection_method,
            k_features=args.k_features,
            variance_threshold=args.variance_threshold,
            lasso_voting_threshold=args.lasso_voting_threshold,
            lasso_n_runs=args.lasso_n_runs,
            elastic_l1_ratio=args.elastic_l1_ratio
        )
        logger.info(f"ç‰¹å¾é€‰æ‹©æ–¹æ³•: {args.feature_selection_method}, æ¯ä¸ªæ¨¡æ€ä¿ç•™: {args.k_features} ä¸ªç‰¹å¾")
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    train_dataset, label_dist = load_data(
        args.data_file, 
        args.modalities, 
        include_ct=not args.no_ct, 
        feature_selectors=feature_selectors,
        training=True
    )
    
    logger.info(f"åŠ è½½è®­ç»ƒæ•°æ®: {args.data_file}, æ ·æœ¬æ•°: {len(train_dataset)}")
    logger.info(f"æ ‡ç­¾åˆ†å¸ƒ: {label_dist}")
    
    # è¿è¡Œè®­ç»ƒ
    if args.k_folds > 1:
        logger.info("è¿è¡Œäº¤å‰éªŒè¯")
        results = cross_validate(args, train_dataset, feature_selectors)
    else:
        logger.info("è¿è¡Œå•æ¬¡è®­ç»ƒ")
        results = run_single_training(args, train_dataset, feature_selectors)
    
    if results:
        logger.info("è®­ç»ƒå®Œæˆ!")
    else:
        logger.error("è®­ç»ƒå¤±è´¥!")


def analyze_cross_fold_errors(args, dataset, feature_selectors, cv_dir, min_error_folds=2):
    """
    è·¨foldé”™è¯¯åˆ†æï¼šç”¨æ¯ä¸ªfoldçš„æ¨¡å‹å¯¹å…¨é‡æ•°æ®é¢„æµ‹ï¼Œæ‰¾å‡ºæŒç»­é¢„æµ‹é”™è¯¯çš„æ ·æœ¬
    
    Args:
        args: å‚æ•°å¯¹è±¡
        dataset: å®Œæ•´æ•°æ®é›†
        feature_selectors: ç‰¹å¾é€‰æ‹©å™¨
        cv_dir: äº¤å‰éªŒè¯ç»“æœç›®å½•
        min_error_folds: æœ€å°‘é”™è¯¯foldæ•°é‡
    """
    import glob
    from collections import defaultdict, Counter
    import matplotlib.pyplot as plt
    
    logger.info("=" * 60)
    logger.info("ğŸ” è·¨Foldæ ·æœ¬é”™è¯¯åˆ†æ")
    logger.info("=" * 60)
    
    # 1. æŸ¥æ‰¾æ‰€æœ‰foldæ¨¡å‹
    model_files = glob.glob(os.path.join(cv_dir, "best_model_fold_*.pth"))
    if not model_files:
        logger.error("æœªæ‰¾åˆ°foldæ¨¡å‹æ–‡ä»¶ï¼")
        return
    
    fold_models = {}
    for model_file in model_files:
        try:
            fold_num = int(os.path.basename(model_file).split('_fold_')[1].split('.pth')[0])
            fold_models[fold_num] = model_file
        except (IndexError, ValueError):
            logger.warning(f"æ— æ³•è§£ææ¨¡å‹æ–‡ä»¶: {model_file}")
    
    logger.info(f"æ‰¾åˆ° {len(fold_models)} ä¸ªfoldæ¨¡å‹: {sorted(fold_models.keys())}")
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    full_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)
    
    # 3. åˆ›å»ºCTæ¨¡å‹
    ct_model = None if args.no_ct else create_ct_model(args.ct_model_path, args.device)
    
    # 4. è·å–ç‰¹å¾ç»´åº¦å¹¶åˆ›å»ºç¼–ç å™¨
    sample = dataset[0]
    feature_dims = {}
    for i, mod in enumerate(args.modalities):
        key = f'X{i}'
        if key in sample:
            feature_dims[mod] = sample[key].shape[0]
    
    encoders = {mod: load_encoder(mod, None, args.latent_dim, feature_dims).to(args.device) 
               for mod in args.modalities if mod in feature_dims}
    
    # 5. åˆ›å»ºä¸»æ¨¡å‹
    model = CrossAttentionFusion(
        dim_latent=args.latent_dim,
        n_modalities=len(args.modalities),
        num_classes=2,  # å‡è®¾äºŒåˆ†ç±»
        use_ct=not args.no_ct,
        use_se_block=not args.no_se_block,
        ct_feature_extractor=ct_model
    ).to(args.device)
    
    # 6. å­˜å‚¨é¢„æµ‹ç»“æœ
    sample_predictions = {}  # sample_idx -> {fold: {'pred': label, 'prob': prob, 'correct': bool}}
    sample_true_labels = {}  # sample_idx -> true_label
    sample_ids = {}  # sample_idx -> original_id
    
    # 7. ç”¨æ¯ä¸ªfoldæ¨¡å‹é¢„æµ‹å…¨é‡æ•°æ®
    for fold_num in sorted(fold_models.keys()):
        model_path = fold_models[fold_num]
        logger.info(f"ä½¿ç”¨ Fold {fold_num} æ¨¡å‹è¿›è¡Œé¢„æµ‹...")
        
        # åŠ è½½æ¨¡å‹æƒé‡
        try:
            model.load_state_dict(torch.load(model_path, map_location=args.device, weights_only=True))
            model.eval()
        except Exception as e:
            logger.error(f"åŠ è½½ Fold {fold_num} æ¨¡å‹å¤±è´¥: {e}")
            continue
        
        # é¢„æµ‹
        sample_idx = 0
        correct_count = 0
        
        with torch.no_grad():
            for batch in full_loader:
                yb = batch['y'].to(args.device)
                batch_size = yb.size(0)
                
                try:
                    # ç‰¹å¾æå–
                    modality_features = []
                    for i, mod in enumerate(args.modalities):
                        if f'X{i}' in batch and mod in encoders:
                            features = encoders[mod](batch[f'X{i}'].to(args.device))
                            modality_features.append(features)
                    
                    if not modality_features:
                        sample_idx += batch_size
                        continue
                    
                    # èåˆå’Œé¢„æµ‹
                    zs = torch.stack(modality_features, dim=1)
                    ct_data = batch.get('CT', None)
                    if ct_data is not None:
                        ct_data = ct_data.to(args.device)
                    
                    logits = model(zs, ct_data)
                    probs = torch.softmax(logits, dim=1)
                    preds = logits.argmax(1)
                    
                    # è®°å½•ç»“æœ
                    for i in range(batch_size):
                        idx = sample_idx + i
                        true_label = yb[i].item()
                        pred_label = preds[i].item()
                        pred_prob = probs[i, 1].item()  # æ­£ç±»æ¦‚ç‡
                        is_correct = pred_label == true_label
                        
                        if is_correct:
                            correct_count += 1
                        
                        # åˆå§‹åŒ–è®°å½•
                        if idx not in sample_predictions:
                            sample_predictions[idx] = {}
                            sample_true_labels[idx] = true_label
                            # ä¿å­˜åŸå§‹æ ·æœ¬ID
                            if 'id' in batch:
                                sample_ids[idx] = batch['id'][i]
                            else:
                                sample_ids[idx] = f"sample_{idx}"
                        
                        # ä¿å­˜é¢„æµ‹
                        sample_predictions[idx][fold_num] = {
                            'pred': pred_label,
                            'prob': pred_prob,
                            'correct': is_correct
                        }
                    
                    sample_idx += batch_size
                    
                except Exception as e:
                    logger.error(f"Fold {fold_num} æ‰¹æ¬¡é¢„æµ‹å¤±è´¥: {e}")
                    sample_idx += batch_size
                    continue
        
        fold_accuracy = correct_count / sample_idx if sample_idx > 0 else 0
        logger.info(f"Fold {fold_num} å‡†ç¡®ç‡: {fold_accuracy:.3f} ({correct_count}/{sample_idx})")
    
    # 7. åˆ†ææŒç»­é”™è¯¯æ ·æœ¬
    logger.info(f"\nåˆ†æåœ¨â‰¥{min_error_folds}ä¸ªfoldä¸­é¢„æµ‹é”™è¯¯çš„æ ·æœ¬...")
    
    consistent_errors = {}
    total_folds = len(fold_models)
    
    for sample_idx, fold_predictions in sample_predictions.items():
        available_folds = len(fold_predictions)
        error_count = sum(1 for pred in fold_predictions.values() if not pred['correct'])
        
        if error_count >= min_error_folds and available_folds >= min_error_folds:
            true_label = sample_true_labels[sample_idx]
            error_rate = error_count / available_folds
            
            pred_labels = [p['pred'] for p in fold_predictions.values()]
            pred_probs = [p['prob'] for p in fold_predictions.values()]
            
            consistent_errors[sample_idx] = {
                'true_label': true_label,
                'error_count': error_count,
                'total_predictions': available_folds,
                'error_rate': error_rate,
                'predictions': pred_labels,
                'avg_prob': np.mean(pred_probs),
                'prediction_consistency': len(set(pred_labels)) == 1,
                'consistent_pred_label': pred_labels[0] if len(set(pred_labels)) == 1 else None
            }
    
    # 8. ç”ŸæˆæŠ¥å‘Š
    if not consistent_errors:
        logger.info("âœ… æœªå‘ç°æŒç»­é”™è¯¯çš„æ ·æœ¬ï¼Œæ•°æ®è´¨é‡è‰¯å¥½ï¼")
        return
    
    logger.info(f"ğŸš¨ æ‰¾åˆ° {len(consistent_errors)} ä¸ªæŒç»­é”™è¯¯æ ·æœ¬")
    
    # åˆ†ç±»ç»Ÿè®¡
    very_high_error = [idx for idx, info in consistent_errors.items() if info['error_rate'] >= 0.8]
    high_error = [idx for idx, info in consistent_errors.items() if info['error_rate'] >= 0.6]
    consistent_wrong = [idx for idx, info in consistent_errors.items() 
                      if info['prediction_consistency'] and info['error_rate'] >= 0.6]
    
    logger.info(f"ğŸ“Š åˆ†æç»“æœ:")
    logger.info(f"   ğŸ”´ æé«˜é”™è¯¯ç‡(â‰¥80%): {len(very_high_error)} ä¸ª")
    logger.info(f"   ğŸŸ  é«˜é”™è¯¯ç‡(â‰¥60%): {len(high_error)} ä¸ª")
    logger.info(f"   ğŸ·ï¸  å¯èƒ½æ ‡ç­¾é”™è¯¯: {len(consistent_wrong)} ä¸ª (é¢„æµ‹ä¸€è‡´ä½†ä¸æ ‡ç­¾ä¸ç¬¦)")
    
    # è¯¦ç»†æŠ¥å‘Š
    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("è·¨Foldæ ·æœ¬é”™è¯¯åˆ†æè¯¦ç»†æŠ¥å‘Š")
    report_lines.append("=" * 80)
    report_lines.append(f"æ€»æ ·æœ¬æ•°: {len(sample_predictions)}")
    report_lines.append(f"åˆ†æfoldæ•°: {total_folds}")
    report_lines.append(f"æŒç»­é”™è¯¯æ ·æœ¬: {len(consistent_errors)}")
    report_lines.append(f"æé«˜é”™è¯¯ç‡(â‰¥80%): {len(very_high_error)} ä¸ª")
    report_lines.append(f"å¯èƒ½æ ‡ç­¾é”™è¯¯: {len(consistent_wrong)} ä¸ª")
    report_lines.append("")
    
    # æœ€ä¸¥é‡çš„æ ·æœ¬
    sorted_errors = sorted(consistent_errors.items(), 
                         key=lambda x: (x[1]['error_rate'], -x[1]['avg_prob']), 
                         reverse=True)
    
    report_lines.append("æœ€ä¸¥é‡çš„é”™è¯¯æ ·æœ¬ (Top 20):")
    report_lines.append("-" * 80)
    for i, (sample_idx, info) in enumerate(sorted_errors[:20]):
        status = "âš ï¸æé«˜" if info['error_rate'] >= 0.8 else "ğŸ”´é«˜"
        consistency = "ä¸€è‡´" if info['prediction_consistency'] else "ä¸ä¸€è‡´"
        pred_info = f"â†’{info['consistent_pred_label']}" if info['prediction_consistency'] else "æ··åˆ"
        
        report_lines.append(f"{i+1:2d}. æ ·æœ¬#{sample_idx:<6} | æ ‡ç­¾:{info['true_label']} {pred_info} | "
                           f"é”™è¯¯ç‡:{info['error_rate']:.1%} ({info['error_count']}/{info['total_predictions']}) | "
                           f"æ¦‚ç‡:{info['avg_prob']:.3f} | {status}é”™è¯¯ | é¢„æµ‹{consistency}")
    
    report_lines.append("")
    report_lines.append("ğŸ”§ æ•°æ®æ¸…æ´—å»ºè®®:")
    report_lines.append("-" * 40)
    
    if very_high_error:
        report_lines.append(f"1. ã€é«˜ä¼˜å…ˆçº§ã€‘æ£€æŸ¥ä»¥ä¸‹ {len(very_high_error)} ä¸ªæé«˜é”™è¯¯ç‡æ ·æœ¬:")
        for idx in very_high_error[:10]:
            info = consistent_errors[idx]
            pred_label = info['consistent_pred_label'] if info['prediction_consistency'] else 'æ··åˆ'
            original_id = sample_ids.get(idx, f"sample_{idx}")
            report_lines.append(f"   æ ·æœ¬ID {original_id}: æ ‡ç­¾{info['true_label']} vs é¢„æµ‹{pred_label} (é”™è¯¯ç‡{info['error_rate']:.1%})")
        if len(very_high_error) > 10:
            report_lines.append(f"   ... è¿˜æœ‰{len(very_high_error)-10}ä¸ª")
    
    if consistent_wrong:
        report_lines.append(f"\n2. ã€æ ‡ç­¾æ£€æŸ¥ã€‘ä»¥ä¸‹ {len(consistent_wrong)} ä¸ªæ ·æœ¬å¯èƒ½æ ‡ç­¾é”™è¯¯:")
        for idx in consistent_wrong[:5]:
            info = consistent_errors[idx]
            original_id = sample_ids.get(idx, f"sample_{idx}")
            report_lines.append(f"   æ ·æœ¬ID {original_id}: æ ‡ç­¾{info['true_label']} â†’ æ‰€æœ‰foldéƒ½é¢„æµ‹ä¸º{info['consistent_pred_label']}")
    
    report_lines.append(f"\n3. ã€é¢„æœŸæ”¹è¿›ã€‘:")
    improvement = len(very_high_error) / len(sample_predictions) * 100
    report_lines.append(f"   ç§»é™¤/ä¿®æ­£æé«˜é”™è¯¯ç‡æ ·æœ¬åï¼Œé¢„æœŸå‡†ç¡®ç‡æå‡: ~{improvement:.1f}%")
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = os.path.join(cv_dir, 'cross_fold_error_analysis.txt')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    # ç®€åŒ–ç‰ˆæ§åˆ¶å°è¾“å‡º
    if very_high_error:
        logger.info(f"\nâš ï¸  å»ºè®®ç«‹å³æ£€æŸ¥ä»¥ä¸‹æé«˜é”™è¯¯ç‡æ ·æœ¬:")
        for i, idx in enumerate(very_high_error[:5]):
            info = consistent_errors[idx]
            pred_label = info['consistent_pred_label'] if info['prediction_consistency'] else 'ä¸ä¸€è‡´'
            original_id = sample_ids.get(idx, f"sample_{idx}")
            logger.info(f"   {i+1}. æ ·æœ¬ID {original_id}: æ ‡ç­¾{info['true_label']} vs é¢„æµ‹{pred_label} (é”™è¯¯{info['error_count']}/{info['total_predictions']}æ¬¡)")
        if len(very_high_error) > 5:
            logger.info(f"   ... è¿˜æœ‰{len(very_high_error)-5}ä¸ªï¼Œè¯¦è§æŠ¥å‘Š")
    
    logger.info(f"âœ… è·¨foldé”™è¯¯åˆ†æå®Œæˆï¼")


if __name__ == "__main__":
    main()