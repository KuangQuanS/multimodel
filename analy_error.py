def analyze_cross_fold_errors(args, dataset, feature_selectors, cv_dir, min_error_folds=2):
    """
    è·¨foldé”™è¯¯åˆ†æï¼šç”¨æ¯ä¸ªfoldçš„æ¨¡å‹å¯¹å…¨é‡æ•°æ®é¢„æµ‹ï¼Œæ‰¾å‡ºæŒç»­é¢„æµ‹é”™è¯¯çš„æ ·æœ¬
    
    Args:
        args: å‚æ•°å¯¹è±¡
        dataset: å®Œæ•´æ•°æ®é›†
        feature_selectors: ç‰¹å¾é€‰æ‹©å™¨
        cv_dir: äº¤å‰éªŒè¯ç»“æœç›®å½•
        min_error_folds: æœ€å°‘é”™è¯¯foldæ•°é‡
    """
    import glob
    from collections import defaultdict, Counter
    
    logger.info("=" * 60)
    logger.info("ğŸ” è·¨Foldæ ·æœ¬é”™è¯¯åˆ†æ")
    logger.info("=" * 60)
    
    # 1. æŸ¥æ‰¾æ‰€æœ‰foldæ¨¡å‹
    model_files = glob.glob(os.path.join(cv_dir, "best_model_fold_*.pth"))
    if not model_files:
        logger.error("æœªæ‰¾åˆ°foldæ¨¡å‹æ–‡ä»¶ï¼")
        return
    
    fold_models = {}
    for model_file in model_files:
        try:
            fold_num = int(os.path.basename(model_file).split('_fold_')[1].split('.pth')[0])
            fold_models[fold_num] = model_file
        except (IndexError, ValueError):
            logger.warning(f"æ— æ³•è§£ææ¨¡å‹æ–‡ä»¶: {model_file}")
    
    logger.info(f"æ‰¾åˆ° {len(fold_models)} ä¸ªfoldæ¨¡å‹: {sorted(fold_models.keys())}")
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    full_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)
    
    # 3. åˆ›å»ºCTæ¨¡å‹
    ct_model = None if args.no_ct else create_ct_model(args.ct_model_path, args.device)
    
    # 4. è·å–ç‰¹å¾ç»´åº¦å¹¶åˆ›å»ºç¼–ç å™¨
    sample = dataset[0]
    feature_dims = {}
    for i, mod in enumerate(args.modalities):
        key = f'X{i}'
        if key in sample:
            feature_dims[mod] = sample[key].shape[0]
    
    encoders = {mod: load_encoder(mod, None, args.latent_dim, feature_dims).to(args.device) 
               for mod in args.modalities if mod in feature_dims}
    
    # 5. åˆ›å»ºä¸»æ¨¡å‹
    model = CrossAttentionFusion(
        dim_latent=args.latent_dim,
        n_modalities=len(args.modalities),
        num_classes=2,  # å‡è®¾äºŒåˆ†ç±»
        use_ct=not args.no_ct,
        ct_feature_extractor=ct_model
    ).to(args.device)
    
    # 6. å­˜å‚¨é¢„æµ‹ç»“æœ
    sample_predictions = {}  # sample_idx -> {fold: {'pred': label, 'prob': prob, 'correct': bool}}
    sample_true_labels = {}  # sample_idx -> true_label
    sample_ids = {}  # sample_idx -> original_id
    
    # 7. ç”¨æ¯ä¸ªfoldæ¨¡å‹é¢„æµ‹å…¨é‡æ•°æ®
    for fold_num in sorted(fold_models.keys()):
        model_path = fold_models[fold_num]
        logger.info(f"ä½¿ç”¨ Fold {fold_num} æ¨¡å‹è¿›è¡Œé¢„æµ‹...")
        
        # åŠ è½½æ¨¡å‹æƒé‡
        try:
            model.load_state_dict(torch.load(model_path, map_location=args.device, weights_only=True))
            model.eval()
        except Exception as e:
            logger.error(f"åŠ è½½ Fold {fold_num} æ¨¡å‹å¤±è´¥: {e}")
            continue
        
        # é¢„æµ‹
        sample_idx = 0
        correct_count = 0
        
        with torch.no_grad():
            for batch in full_loader:
                yb = batch['y'].to(args.device)
                batch_size = yb.size(0)
                
                try:
                    # ç‰¹å¾æå–
                    modality_features = []
                    for i, mod in enumerate(args.modalities):
                        if f'X{i}' in batch and mod in encoders:
                            features = encoders[mod](batch[f'X{i}'].to(args.device))
                            modality_features.append(features)
                    
                    if not modality_features:
                        sample_idx += batch_size
                        continue
                    
                    # èåˆå’Œé¢„æµ‹
                    zs = torch.stack(modality_features, dim=1)
                    ct_data = batch.get('CT', None)
                    if ct_data is not None:
                        ct_data = ct_data.to(args.device)
                    
                    logits = model(zs, ct_data)
                    probs = torch.softmax(logits, dim=1)
                    preds = logits.argmax(1)
                    
                    # è®°å½•ç»“æœ
                    for i in range(batch_size):
                        idx = sample_idx + i
                        true_label = yb[i].item()
                        pred_label = preds[i].item()
                        pred_prob = probs[i, 1].item()  # æ­£ç±»æ¦‚ç‡
                        is_correct = pred_label == true_label
                        
                        if is_correct:
                            correct_count += 1
                        
                        # åˆå§‹åŒ–è®°å½•
                        if idx not in sample_predictions:
                            sample_predictions[idx] = {}
                            sample_true_labels[idx] = true_label
                            # ä¿å­˜åŸå§‹æ ·æœ¬ID
                            if 'id' in batch:
                                sample_ids[idx] = batch['id'][i]
                            else:
                                sample_ids[idx] = f"sample_{idx}"
                        
                        # ä¿å­˜é¢„æµ‹
                        sample_predictions[idx][fold_num] = {
                            'pred': pred_label,
                            'prob': pred_prob,
                            'correct': is_correct
                        }
                    
                    sample_idx += batch_size
                    
                except Exception as e:
                    logger.error(f"Fold {fold_num} æ‰¹æ¬¡é¢„æµ‹å¤±è´¥: {e}")
                    sample_idx += batch_size
                    continue
        
        fold_accuracy = correct_count / sample_idx if sample_idx > 0 else 0
        logger.info(f"Fold {fold_num} å‡†ç¡®ç‡: {fold_accuracy:.3f} ({correct_count}/{sample_idx})")
    
    # 7. åˆ†ææŒç»­é”™è¯¯æ ·æœ¬
    logger.info(f"\nåˆ†æåœ¨â‰¥{min_error_folds}ä¸ªfoldä¸­é¢„æµ‹é”™è¯¯çš„æ ·æœ¬...")
    
    consistent_errors = {}
    total_folds = len(fold_models)
    
    for sample_idx, fold_predictions in sample_predictions.items():
        available_folds = len(fold_predictions)
        error_count = sum(1 for pred in fold_predictions.values() if not pred['correct'])
        
        if error_count >= min_error_folds and available_folds >= min_error_folds:
            true_label = sample_true_labels[sample_idx]
            error_rate = error_count / available_folds
            
            pred_labels = [p['pred'] for p in fold_predictions.values()]
            pred_probs = [p['prob'] for p in fold_predictions.values()]
            
            consistent_errors[sample_idx] = {
                'true_label': true_label,
                'error_count': error_count,
                'total_predictions': available_folds,
                'error_rate': error_rate,
                'predictions': pred_labels,
                'avg_prob': np.mean(pred_probs),
                'prediction_consistency': len(set(pred_labels)) == 1,
                'consistent_pred_label': pred_labels[0] if len(set(pred_labels)) == 1 else None
            }
    
    # 8. ç”ŸæˆæŠ¥å‘Š
    if not consistent_errors:
        logger.info("âœ… æœªå‘ç°æŒç»­é”™è¯¯çš„æ ·æœ¬ï¼Œæ•°æ®è´¨é‡è‰¯å¥½ï¼")
        return
    
    logger.info(f"ğŸš¨ æ‰¾åˆ° {len(consistent_errors)} ä¸ªæŒç»­é”™è¯¯æ ·æœ¬")
    
    # åˆ†ç±»ç»Ÿè®¡
    very_high_error = [idx for idx, info in consistent_errors.items() if info['error_rate'] >= 0.8]
    high_error = [idx for idx, info in consistent_errors.items() if info['error_rate'] >= 0.6]
    consistent_wrong = [idx for idx, info in consistent_errors.items() 
                      if info['prediction_consistency'] and info['error_rate'] >= 0.6]
    
    logger.info(f"ğŸ“Š åˆ†æç»“æœ:")
    logger.info(f"   ğŸ”´ æé«˜é”™è¯¯ç‡(â‰¥80%): {len(very_high_error)} ä¸ª")
    logger.info(f"   ğŸŸ  é«˜é”™è¯¯ç‡(â‰¥60%): {len(high_error)} ä¸ª")
    logger.info(f"   ğŸ·ï¸  å¯èƒ½æ ‡ç­¾é”™è¯¯: {len(consistent_wrong)} ä¸ª (é¢„æµ‹ä¸€è‡´ä½†ä¸æ ‡ç­¾ä¸ç¬¦)")
    
    # è¯¦ç»†æŠ¥å‘Š
    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("è·¨Foldæ ·æœ¬é”™è¯¯åˆ†æè¯¦ç»†æŠ¥å‘Š")
    report_lines.append("=" * 80)
    report_lines.append(f"æ€»æ ·æœ¬æ•°: {len(sample_predictions)}")
    report_lines.append(f"åˆ†æfoldæ•°: {total_folds}")
    report_lines.append(f"æŒç»­é”™è¯¯æ ·æœ¬: {len(consistent_errors)}")
    report_lines.append(f"æé«˜é”™è¯¯ç‡(â‰¥80%): {len(very_high_error)} ä¸ª")
    report_lines.append(f"å¯èƒ½æ ‡ç­¾é”™è¯¯: {len(consistent_wrong)} ä¸ª")
    report_lines.append("")
    
    # æœ€ä¸¥é‡çš„æ ·æœ¬
    sorted_errors = sorted(consistent_errors.items(), 
                         key=lambda x: (x[1]['error_rate'], -x[1]['avg_prob']), 
                         reverse=True)
    
    report_lines.append("æœ€ä¸¥é‡çš„é”™è¯¯æ ·æœ¬ (Top 20):")
    report_lines.append("-" * 80)
    for i, (sample_idx, info) in enumerate(sorted_errors[:20]):
        status = "âš ï¸æé«˜" if info['error_rate'] >= 0.8 else "ğŸ”´é«˜"
        consistency = "ä¸€è‡´" if info['prediction_consistency'] else "ä¸ä¸€è‡´"
        pred_info = f"â†’{info['consistent_pred_label']}" if info['prediction_consistency'] else "æ··åˆ"
        
        report_lines.append(f"{i+1:2d}. æ ·æœ¬#{sample_idx:<6} | æ ‡ç­¾:{info['true_label']} {pred_info} | "
                           f"é”™è¯¯ç‡:{info['error_rate']:.1%} ({info['error_count']}/{info['total_predictions']}) | "
                           f"æ¦‚ç‡:{info['avg_prob']:.3f} | {status}é”™è¯¯ | é¢„æµ‹{consistency}")
    
    report_lines.append("")
    report_lines.append("ğŸ”§ æ•°æ®æ¸…æ´—å»ºè®®:")
    report_lines.append("-" * 40)
    
    if very_high_error:
        report_lines.append(f"1. ã€é«˜ä¼˜å…ˆçº§ã€‘æ£€æŸ¥ä»¥ä¸‹ {len(very_high_error)} ä¸ªæé«˜é”™è¯¯ç‡æ ·æœ¬:")
        for idx in very_high_error[:10]:
            info = consistent_errors[idx]
            pred_label = info['consistent_pred_label'] if info['prediction_consistency'] else 'æ··åˆ'
            original_id = sample_ids.get(idx, f"sample_{idx}")
            report_lines.append(f"   æ ·æœ¬ID {original_id}: æ ‡ç­¾{info['true_label']} vs é¢„æµ‹{pred_label} (é”™è¯¯ç‡{info['error_rate']:.1%})")
        if len(very_high_error) > 10:
            report_lines.append(f"   ... è¿˜æœ‰{len(very_high_error)-10}ä¸ª")
    
    if consistent_wrong:
        report_lines.append(f"\n2. ã€æ ‡ç­¾æ£€æŸ¥ã€‘ä»¥ä¸‹ {len(consistent_wrong)} ä¸ªæ ·æœ¬å¯èƒ½æ ‡ç­¾é”™è¯¯:")
        for idx in consistent_wrong[:5]:
            info = consistent_errors[idx]
            original_id = sample_ids.get(idx, f"sample_{idx}")
            report_lines.append(f"   æ ·æœ¬ID {original_id}: æ ‡ç­¾{info['true_label']} â†’ æ‰€æœ‰foldéƒ½é¢„æµ‹ä¸º{info['consistent_pred_label']}")
    
    report_lines.append(f"\n3. ã€é¢„æœŸæ”¹è¿›ã€‘:")
    improvement = len(very_high_error) / len(sample_predictions) * 100
    report_lines.append(f"   ç§»é™¤/ä¿®æ­£æé«˜é”™è¯¯ç‡æ ·æœ¬åï¼Œé¢„æœŸå‡†ç¡®ç‡æå‡: ~{improvement:.1f}%")
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = os.path.join(cv_dir, 'cross_fold_error_analysis.txt')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    # ç®€åŒ–ç‰ˆæ§åˆ¶å°è¾“å‡º
    if very_high_error:
        logger.info(f"\nâš ï¸  å»ºè®®ç«‹å³æ£€æŸ¥ä»¥ä¸‹æé«˜é”™è¯¯ç‡æ ·æœ¬:")
        for i, idx in enumerate(very_high_error[:5]):
            info = consistent_errors[idx]
            pred_label = info['consistent_pred_label'] if info['prediction_consistency'] else 'ä¸ä¸€è‡´'
            original_id = sample_ids.get(idx, f"sample_{idx}")
            logger.info(f"   {i+1}. æ ·æœ¬ID {original_id}: æ ‡ç­¾{info['true_label']} vs é¢„æµ‹{pred_label} (é”™è¯¯{info['error_count']}/{info['total_predictions']}æ¬¡)")
        if len(very_high_error) > 5:
            logger.info(f"   ... è¿˜æœ‰{len(very_high_error)-5}ä¸ªï¼Œè¯¦è§æŠ¥å‘Š")
    
    logger.info(f"âœ… è·¨foldé”™è¯¯åˆ†æå®Œæˆï¼")