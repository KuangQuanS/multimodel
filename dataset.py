"""
æ•°æ®åŠ è½½å’Œå¤„ç†æ¨¡å—
"""
import numpy as np
import torch
from torch.utils.data import Dataset
from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.linear_model import ElasticNetCV
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import torchvision.transforms as transforms
from scipy import ndimage
import random


class ElasticNetVotingSelector(BaseEstimator, TransformerMixin):
    """ElasticNetæŠ•ç¥¨ç‰¹å¾é€‰æ‹©å™¨
    
    æµç¨‹ï¼š
    1. æ–¹å·®è¿‡æ»¤ï¼šç§»é™¤ä½æ–¹å·®ç‰¹å¾
    2. å¤šæ¬¡ElasticNetï¼šè¿è¡Œå¤šæ¬¡ElasticNetå›å½’ï¼Œæ¯æ¬¡è®°å½•é€‰æ‹©çš„ç‰¹å¾
    3. æŠ•ç¥¨ç»Ÿè®¡ï¼šç»Ÿè®¡æ¯ä¸ªç‰¹å¾è¢«é€‰ä¸­çš„æ¬¡æ•°
    4. é˜ˆå€¼è¿‡æ»¤ï¼šä¿ç•™æŠ•ç¥¨æ¯”ä¾‹ >= é˜ˆå€¼çš„ç‰¹å¾
    5. Topç‰¹å¾ï¼šä»é€šè¿‡é˜ˆå€¼çš„ç‰¹å¾ä¸­é€‰æ‹©Top Kä¸ª
    
    ElasticNet = Î± * L1_ratio * ||w||_1 + 0.5 * Î± * (1 - L1_ratio) * ||w||_2^2
    L1_ratio=1.0 ç›¸å½“äºLASSO, L1_ratio=0.0 ç›¸å½“äºRidge
    """
    
    def __init__(self, variance_threshold=0.01, n_runs=10, voting_threshold=0.6, 
                 k_features=200, l1_ratio=0.7, random_state=42):
        self.variance_threshold = variance_threshold
        self.n_runs = n_runs
        self.voting_threshold = voting_threshold
        self.k_features = k_features
        self.l1_ratio = l1_ratio  # ElasticNetçš„L1/L2æ··åˆæ¯”ä¾‹
        self.random_state = random_state
        
    def fit(self, X, y):
        """è®­ç»ƒç‰¹å¾é€‰æ‹©å™¨"""
        np.random.seed(self.random_state)
        
        # Step 1: æ–¹å·®è¿‡æ»¤
        self.variance_selector_ = VarianceThreshold(threshold=self.variance_threshold)
        X_variance_filtered = self.variance_selector_.fit_transform(X)
        
        print(f"æ–¹å·®è¿‡æ»¤: {X.shape[1]} -> {X_variance_filtered.shape[1]} ç‰¹å¾")
        
        # Step 2-4: å¤šæ¬¡ElasticNetæŠ•ç¥¨
        n_features = X_variance_filtered.shape[1]
        vote_counts = np.zeros(n_features)
        
        # æ ‡å‡†åŒ–æ•°æ®ç”¨äºElasticNet
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_variance_filtered)
        
        print(f"ğŸ” ElasticNetæŠ•ç¥¨ç‰¹å¾é€‰æ‹© ({self.n_runs}æ¬¡è¿è¡Œ)...")
        successful_runs = 0
        
        for i in range(self.n_runs):
            # æ¯æ¬¡ä½¿ç”¨ä¸åŒçš„éšæœºçŠ¶æ€å’ŒL1_ratio
            try:
                # åœ¨l1_ratioå‘¨å›´æ·»åŠ ä¸€äº›éšæœºæ€§ï¼Œå¢åŠ å¤šæ ·æ€§
                current_l1_ratio = max(0.1, min(0.9, self.l1_ratio + np.random.normal(0, 0.1)))
                
                elastic_net = ElasticNetCV(
                    l1_ratio=current_l1_ratio,  # L1/L2æ··åˆæ¯”ä¾‹
                    cv=5, 
                    random_state=self.random_state + i, 
                    max_iter=10000,
                    tol=1e-3,  # æ›´å®½æ¾çš„æ”¶æ•›å®¹å¿åº¦
                    selection='random',  # éšæœºç‰¹å¾é€‰æ‹©ï¼Œæé«˜æ”¶æ•›æ€§
                    alphas=np.logspace(-4, -1, 50)
                )
                
                # å¿½ç•¥æ”¶æ•›è­¦å‘Šå’Œå…¶ä»–è­¦å‘Š
                import warnings
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore")  # å¿½ç•¥æ‰€æœ‰è­¦å‘Š
                    elastic_net.fit(X_scaled, y)
                
                # ç»Ÿè®¡éé›¶ç³»æ•°çš„ç‰¹å¾
                selected_features = np.abs(elastic_net.coef_) > 1e-6
                n_selected = np.sum(selected_features)
                vote_counts += selected_features
                successful_runs += 1
                
                # ç®€åŒ–è¾“å‡ºï¼šåªåœ¨å…³é”®ç‚¹æ˜¾ç¤ºç»“æœ
                if i == 0 or (i + 1) % max(1, self.n_runs // 3) == 0 or i == self.n_runs - 1:
                    print(f"  è¿è¡Œ {i+1:2d}/{self.n_runs}: é€‰æ‹©äº† {n_selected:4d} ç‰¹å¾")
                
            except Exception as e:
                # å¦‚æœElasticNetå¤±è´¥ï¼Œä½¿ç”¨éšæœºé€‰æ‹©ä½œä¸ºfallback
                n_random = min(self.k_features, n_features // 2)
                random_indices = np.random.choice(n_features, n_random, replace=False)
                random_features = np.zeros(n_features, dtype=bool)
                random_features[random_indices] = True
                vote_counts += random_features
                if i == 0 or (i + 1) % max(1, self.n_runs // 3) == 0:
                    print(f"  è¿è¡Œ {i+1:2d}/{self.n_runs}: æ”¶æ•›å¤±è´¥ï¼Œä½¿ç”¨éšæœºé€‰æ‹©")
            
        # Step 4: è®¡ç®—æŠ•ç¥¨æ¯”ä¾‹å¹¶åº”ç”¨é˜ˆå€¼
        vote_ratios = vote_counts / self.n_runs
        threshold_mask = vote_ratios >= self.voting_threshold
        
        # ç®€åŒ–æŠ•ç¥¨ç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ“Š ElasticNetæŠ•ç¥¨å®Œæˆ: æˆåŠŸ{successful_runs}/{self.n_runs}æ¬¡, æœ€é«˜æŠ•ç¥¨ç‡{np.max(vote_ratios):.3f}")
        print(f"ğŸ¯ é˜ˆå€¼è¿‡æ»¤(>={self.voting_threshold}): {np.sum(threshold_mask):,} ç‰¹å¾é€šè¿‡")
        
        # Step 5: ä»é€šè¿‡é˜ˆå€¼çš„ç‰¹å¾ä¸­é€‰æ‹©Top Kä¸ª
        if np.sum(threshold_mask) > self.k_features:
            # æŒ‰æŠ•ç¥¨æ¯”ä¾‹æ’åºï¼Œé€‰æ‹©Top Kä¸ª
            threshold_indices = np.where(threshold_mask)[0]
            threshold_ratios = vote_ratios[threshold_mask]
            top_k_indices = threshold_indices[np.argsort(threshold_ratios)[::-1][:self.k_features]]
            
            final_mask = np.zeros(n_features, dtype=bool)
            final_mask[top_k_indices] = True
            self.selected_features_ = final_mask
            
            print(f"âœ‚ï¸ Top-Ké€‰æ‹©: ä¿ç•™å‰{self.k_features:,}ä¸ªç‰¹å¾")
        else:
            # å¦‚æœé€šè¿‡é˜ˆå€¼çš„ç‰¹å¾ä¸è¶³Kä¸ªï¼Œå…¨éƒ¨ä¿ç•™
            self.selected_features_ = threshold_mask
            print(f"âœ‚ï¸ ä¿ç•™æ‰€æœ‰é€šè¿‡é˜ˆå€¼çš„{np.sum(threshold_mask):,}ä¸ªç‰¹å¾")
        
        print(f"âœ… æœ€ç»ˆé€‰æ‹©: {np.sum(self.selected_features_):,}ä¸ªç‰¹å¾")
        
        # ä¿å­˜æŠ•ç¥¨ç»Ÿè®¡ä¿¡æ¯ç”¨äºåˆ†æ
        self.vote_ratios_ = vote_ratios
        self.scaler_ = scaler
        
        return self
    
    def transform(self, X):
        """åº”ç”¨ç‰¹å¾é€‰æ‹©"""
        # å…ˆåº”ç”¨æ–¹å·®è¿‡æ»¤
        X_variance_filtered = self.variance_selector_.transform(X)
        # å†åº”ç”¨ElasticNetæŠ•ç¥¨é€‰æ‹©
        return X_variance_filtered[:, self.selected_features_]
    
    def get_support(self, indices=False):
        """è¿”å›è¢«é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•æˆ–mask"""
        # éœ€è¦å°†ä¸¤æ­¥é€‰æ‹©çš„ç»“æœåˆå¹¶
        variance_support = self.variance_selector_.get_support()
        final_support = np.zeros(len(variance_support), dtype=bool)
        final_support[variance_support] = self.selected_features_
        
        if indices:
            return np.where(final_support)[0]
        return final_support


def apply_ct_augmentation(ct_data, training=True, strong_augment=False):
    """
    ä¸ºCTæ•°æ®åº”ç”¨æ•°æ®å¢å¼º
    
    Args:
        ct_data: CTæ•°æ® numpy array, shape (H, W, C) æˆ– (C, H, W)
        training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        strong_augment: æ˜¯å¦ä½¿ç”¨å¼ºå¢å¼º
    
    Returns:
        augmented_ct_data: å¢å¼ºåçš„CTæ•°æ®
    """
    if not training:
        return ct_data.copy()  # ç¡®ä¿è¿”å›è¿ç»­æ•°ç»„
    
    # ç¡®ä¿æ•°æ®æ˜¯è¿ç»­çš„ä¸”æ˜¯ (H, W, C) æ ¼å¼
    if ct_data.shape[0] == 3:  # å¦‚æœæ˜¯ (3, 64, 64)
        ct_data = np.ascontiguousarray(ct_data.transpose(1, 2, 0))  # è½¬æ¢ä¸º (64, 64, 3)
    else:
        ct_data = np.ascontiguousarray(ct_data)
    
    H, W, C = ct_data.shape
    augmented = ct_data.copy()
    
    if strong_augment:
        return apply_strong_ct_augmentation(augmented)
    
    # åŸå§‹çš„è½»åº¦å¢å¼º
    # 1. éšæœºæ—‹è½¬ (-10Â° to +10Â°)
    if random.random() < 0.5:
        angle = random.uniform(-10, 10)
        for c in range(C):
            rotated = ndimage.rotate(augmented[:, :, c], angle, 
                                   reshape=False, mode='nearest')
            augmented[:, :, c] = np.ascontiguousarray(rotated)
    
    # 2. éšæœºå¹³ç§» (Â±5 pixels)
    if random.random() < 0.5:
        shift_x = random.randint(-5, 5)
        shift_y = random.randint(-5, 5)
        for c in range(C):
            shifted = ndimage.shift(augmented[:, :, c], (shift_y, shift_x), 
                                  mode='nearest')
            augmented[:, :, c] = np.ascontiguousarray(shifted)
    
    # 3. éšæœºäº®åº¦è°ƒæ•´ (Â±10%)
    if random.random() < 0.5:
        brightness_factor = random.uniform(0.9, 1.1)
        augmented = np.ascontiguousarray(np.clip(augmented * brightness_factor, 0, 1))
    
    # 4. éšæœºå¯¹æ¯”åº¦è°ƒæ•´ (Â±15%)
    if random.random() < 0.5:
        contrast_factor = random.uniform(0.85, 1.15)
        mean_val = augmented.mean()
        augmented = np.ascontiguousarray(np.clip((augmented - mean_val) * contrast_factor + mean_val, 0, 1))
    
    # 5. éšæœºå™ªå£°æ·»åŠ  (å¾®å¼±é«˜æ–¯å™ªå£°)
    if random.random() < 0.3:
        noise = np.random.normal(0, 0.02, augmented.shape)
        augmented = np.ascontiguousarray(np.clip(augmented + noise, 0, 1))
    
    # 6. éšæœºæ°´å¹³ç¿»è½¬
    if random.random() < 0.5:
        augmented = np.ascontiguousarray(np.flip(augmented, axis=1))
    
    # 7. éšæœºç¼©æ”¾ (0.95-1.05å€) - ç®€åŒ–å®ç°é¿å…å¤æ‚æ“ä½œ
    if random.random() < 0.2:  # é™ä½æ¦‚ç‡é¿å…è¿‡åº¦å¢å¼º
        scale_factor = random.uniform(0.98, 1.02)  # æ›´å°çš„ç¼©æ”¾èŒƒå›´
        for c in range(C):
            original_slice = augmented[:, :, c]
            zoomed_slice = ndimage.zoom(original_slice, scale_factor, 
                                       mode='nearest', order=1)
            zoomed_slice = np.ascontiguousarray(zoomed_slice)
            
            # ç®€å•çš„ä¸­å¿ƒè£å‰ªæˆ–å¡«å……
            current_h, current_w = zoomed_slice.shape
            if current_h == H and current_w == W:
                augmented[:, :, c] = zoomed_slice
            elif current_h >= H and current_w >= W:
                # ä¸­å¿ƒè£å‰ª
                start_h = (current_h - H) // 2
                start_w = (current_w - W) // 2
                augmented[:, :, c] = zoomed_slice[start_h:start_h+H, start_w:start_w+W]
            else:
                # ä¸­å¿ƒå¡«å……
                padded = np.zeros((H, W), dtype=augmented.dtype)
                start_h = max(0, (H - current_h) // 2)
                start_w = max(0, (W - current_w) // 2)
                end_h = min(H, start_h + current_h)
                end_w = min(W, start_w + current_w)
                
                src_end_h = min(current_h, end_h - start_h)
                src_end_w = min(current_w, end_w - start_w)
                
                padded[start_h:end_h, start_w:end_w] = zoomed_slice[:src_end_h, :src_end_w]
                augmented[:, :, c] = padded
    
    # ç¡®ä¿æœ€ç»ˆç»“æœæ˜¯è¿ç»­çš„
    return np.ascontiguousarray(augmented)


def apply_strong_ct_augmentation(ct_data):
    """
    åº”ç”¨å¼ºCTæ•°æ®å¢å¼º
    
    åŒ…æ‹¬æ›´æ¿€è¿›çš„å˜æ¢ï¼š
    1. æ›´å¤§èŒƒå›´çš„æ—‹è½¬å’Œå¹³ç§»
    2. æ›´å¼ºçš„äº®åº¦å’Œå¯¹æ¯”åº¦å˜åŒ–
    3. å¼¹æ€§å˜å½¢
    4. Cutout/æ“¦é™¤
    5. å¤šç§å™ªå£°
    6. æ··åˆå¢å¼ºï¼ˆMixUpé£æ ¼çš„å˜æ¢ï¼‰
    """
    H, W, C = ct_data.shape
    augmented = ct_data.copy()
    
    # 1. å¼ºæ—‹è½¬ (-30Â° to +30Â°)
    if random.random() < 0.7:  # æ›´é«˜æ¦‚ç‡
        angle = random.uniform(-30, 30)  # æ›´å¤§è§’åº¦èŒƒå›´
        for c in range(C):
            rotated = ndimage.rotate(augmented[:, :, c], angle, 
                                   reshape=False, mode='nearest')
            augmented[:, :, c] = np.ascontiguousarray(rotated)
    
    # 2. å¼ºå¹³ç§» (Â±15 pixels)
    if random.random() < 0.7:
        shift_x = random.randint(-15, 15)
        shift_y = random.randint(-15, 15)
        for c in range(C):
            shifted = ndimage.shift(augmented[:, :, c], (shift_y, shift_x), 
                                  mode='nearest')
            augmented[:, :, c] = np.ascontiguousarray(shifted)
    
    # 3. å¼ºäº®åº¦è°ƒæ•´ (Â±30%)
    if random.random() < 0.6:
        brightness_factor = random.uniform(0.7, 1.3)
        augmented = np.ascontiguousarray(np.clip(augmented * brightness_factor, 0, 1))
    
    # 4. å¼ºå¯¹æ¯”åº¦è°ƒæ•´ (Â±40%)
    if random.random() < 0.6:
        contrast_factor = random.uniform(0.6, 1.4)
        mean_val = np.nanmean(augmented)  # ä½¿ç”¨nanmeané¿å…NaN
        if np.isfinite(mean_val):  # åªæœ‰mean_valæ˜¯æœ‰é™å€¼æ—¶æ‰åº”ç”¨å¯¹æ¯”åº¦è°ƒæ•´
            augmented = np.ascontiguousarray(np.clip((augmented - mean_val) * contrast_factor + mean_val, 0, 1))
    
    # 5. å¤šç§å™ªå£°
    if random.random() < 0.5:
        noise_type = random.choice(['gaussian', 'uniform', 'salt_pepper'])
        
        if noise_type == 'gaussian':
            # é«˜æ–¯å™ªå£°
            noise_std = random.uniform(0.02, 0.08)
            noise = np.random.normal(0, noise_std, augmented.shape)
            augmented = np.ascontiguousarray(np.clip(augmented + noise, 0, 1))
            
        elif noise_type == 'uniform':
            # å‡åŒ€å™ªå£°
            noise_range = random.uniform(0.02, 0.06)
            noise = np.random.uniform(-noise_range, noise_range, augmented.shape)
            augmented = np.ascontiguousarray(np.clip(augmented + noise, 0, 1))
            
        elif noise_type == 'salt_pepper':
            # æ¤’ç›å™ªå£°
            noise_prob = random.uniform(0.01, 0.05)
            mask = np.random.random(augmented.shape) < noise_prob
            augmented[mask] = np.random.choice([0, 1], size=np.sum(mask))
            augmented = np.ascontiguousarray(augmented)
    
    # 6. å¼¹æ€§å˜å½¢ (ç®€åŒ–ç‰ˆ)
    if random.random() < 0.3:
        alpha = random.uniform(10, 30)  # å˜å½¢å¼ºåº¦
        sigma = random.uniform(3, 6)    # å¹³æ»‘å‚æ•°
        
        for c in range(C):
            # ç”Ÿæˆéšæœºä½ç§»åœº
            dx = ndimage.gaussian_filter((np.random.rand(H, W) - 0.5), sigma) * alpha
            dy = ndimage.gaussian_filter((np.random.rand(H, W) - 0.5), sigma) * alpha
            
            # ç”Ÿæˆç½‘æ ¼
            x, y = np.meshgrid(np.arange(W), np.arange(H))
            indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1))
            
            # åº”ç”¨å˜å½¢
            deformed = ndimage.map_coordinates(augmented[:, :, c], indices, 
                                             order=1, mode='nearest').reshape(H, W)
            augmented[:, :, c] = np.ascontiguousarray(deformed)
    
    # 7. Cutout/Random Erasing
    if random.random() < 0.4:
        n_holes = random.randint(1, 3)  # 1-3ä¸ªæ“¦é™¤åŒºåŸŸ
        
        for _ in range(n_holes):
            hole_size = random.randint(8, 16)  # æ“¦é™¤åŒºåŸŸå¤§å°
            y1 = random.randint(0, H - hole_size)
            x1 = random.randint(0, W - hole_size)
            y2 = y1 + hole_size
            x2 = x1 + hole_size
            
            # éšæœºå¡«å……å€¼
            fill_value = random.uniform(0, 1)
            augmented[y1:y2, x1:x2, :] = fill_value
    
    # 8. éšæœºç¿»è½¬ï¼ˆåŒ…æ‹¬å‚ç›´ç¿»è½¬ï¼‰
    if random.random() < 0.5:
        augmented = np.ascontiguousarray(np.flip(augmented, axis=1))  # æ°´å¹³ç¿»è½¬
    
    if random.random() < 0.3:
        augmented = np.ascontiguousarray(np.flip(augmented, axis=0))  # å‚ç›´ç¿»è½¬
    
    # 9. å¼ºç¼©æ”¾ (0.8-1.3å€)
    if random.random() < 0.5:
        scale_factor = random.uniform(0.8, 1.3)
        for c in range(C):
            original_slice = augmented[:, :, c]
            zoomed_slice = ndimage.zoom(original_slice, scale_factor, 
                                       mode='nearest', order=1)
            zoomed_slice = np.ascontiguousarray(zoomed_slice)
            
            current_h, current_w = zoomed_slice.shape
            if current_h >= H and current_w >= W:
                # ä¸­å¿ƒè£å‰ª
                start_h = (current_h - H) // 2
                start_w = (current_w - W) // 2
                augmented[:, :, c] = zoomed_slice[start_h:start_h+H, start_w:start_w+W]
            else:
                # ä¸­å¿ƒå¡«å……
                padded = np.zeros((H, W), dtype=augmented.dtype)
                start_h = (H - current_h) // 2
                start_w = (W - current_w) // 2
                end_h = start_h + current_h
                end_w = start_w + current_w
                
                # ç¡®ä¿è¾¹ç•Œä¸è¶Šç•Œ
                start_h = max(0, start_h)
                start_w = max(0, start_w)
                end_h = min(H, end_h)
                end_w = min(W, end_w)
                
                padded[start_h:end_h, start_w:end_w] = zoomed_slice[:end_h-start_h, :end_w-start_w]
                augmented[:, :, c] = padded
    
    # 10. Gammaå˜æ¢ï¼ˆæ¨¡æ‹Ÿä¸åŒçš„æˆåƒæ¡ä»¶ï¼‰
    if random.random() < 0.3:
        gamma = random.uniform(0.5, 2.0)
        # ç¡®ä¿æ•°æ®ä¸¥æ ¼éè´Ÿï¼Œé¿å…0^è´Ÿæ•°çš„æƒ…å†µ
        augmented = np.clip(augmented, 1e-8, 1.0)  # é¿å…å®Œå…¨çš„0å€¼
        
        # å®‰å…¨çš„å¹‚è¿ç®—
        try:
            augmented = np.power(augmented, gamma)
            # æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ
            if np.isnan(augmented).any() or np.isinf(augmented).any():
                # å¦‚æœæœ‰é—®é¢˜ï¼Œè·³è¿‡gammaå˜æ¢
                augmented = np.clip(ct_data.copy(), 0, 1)
        except (RuntimeWarning, FloatingPointError):
            # å¦‚æœå‡ºç°æ•°å€¼é—®é¢˜ï¼Œè·³è¿‡gammaå˜æ¢
            augmented = np.clip(ct_data.copy(), 0, 1)
        
        augmented = np.ascontiguousarray(augmented)
    
    # 11. è‰²å½©æŠ–åŠ¨ï¼ˆå¯¹äºå¤šé€šé“CTï¼‰
    if random.random() < 0.3 and C > 1:
        for c in range(C):
            # é€šé“ç‰¹å®šçš„äº®åº¦è°ƒæ•´
            channel_factor = random.uniform(0.8, 1.2)
            augmented[:, :, c] = np.clip(augmented[:, :, c] * channel_factor, 0, 1)
    
    # 12. å±€éƒ¨å¯¹æ¯”åº¦è°ƒæ•´ï¼ˆCLAHEé£æ ¼ï¼‰
    if random.random() < 0.2:
        for c in range(C):
            # ç®€åŒ–ç‰ˆçš„è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡
            slice_data = augmented[:, :, c]
            # åˆ†å—å¤„ç†
            block_size = 16
            for i in range(0, H, block_size):
                for j in range(0, W, block_size):
                    block = slice_data[i:min(i+block_size, H), j:min(j+block_size, W)]
                    if block.size > 0:
                        # å±€éƒ¨å¯¹æ¯”åº¦å¢å¼º
                        block_mean = block.mean()
                        block_std = block.std()
                        if block_std > 0:
                            enhanced_block = (block - block_mean) * random.uniform(1.0, 1.5) + block_mean
                            slice_data[i:min(i+block_size, H), j:min(j+block_size, W)] = np.clip(enhanced_block, 0, 1)
    
    # æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿æ²¡æœ‰NaNã€infæˆ–è¶…å‡ºèŒƒå›´çš„å€¼
    augmented = np.nan_to_num(augmented, nan=0.0, posinf=1.0, neginf=0.0)
    augmented = np.clip(augmented, 0, 1)
    
    return np.ascontiguousarray(augmented)



class MultiModalDataset(Dataset):
    """å¤šæ¨¡æ€æ•°æ®é›†ç±»ï¼Œæ”¯æŒcfDNAå’ŒCTæ•°æ®"""
    def __init__(self, npz_path, modalities, indices=None, include_ct=False, feature_selectors=None, training=True, strong_augment=False):
        data = np.load(npz_path, allow_pickle=True)  # ensure support for string arrays
        self.modalities = modalities
        self.Xs = [data[mod] for mod in modalities]
        self.y = data['y']
        self.include_ct = include_ct
        self.ct_data = data['CT'] if include_ct and 'CT' in data else None
        self.id = data['id'] if 'id' in data else None
        self.training = training  # è®­ç»ƒæ¨¡å¼æ ‡å¿—ï¼Œç”¨äºæ§åˆ¶æ•°æ®å¢å¼º
        self.strong_augment = strong_augment  # å¼ºå¢å¼ºæ ‡å¿—
        
        # åº”ç”¨ç‰¹å¾é€‰æ‹©
        self.feature_selectors = feature_selectors or {}
        if self.feature_selectors:
            for i, mod in enumerate(modalities):
                if mod in self.feature_selectors:
                    self.Xs[i] = self.feature_selectors[mod].transform(self.Xs[i])

        if indices is not None:
            self.Xs = [x[indices] for x in self.Xs]
            self.y = self.y[indices]
            if self.ct_data is not None:
                self.ct_data = self.ct_data[indices]
            if self.id is not None:
                self.id = self.id[indices]

    def __len__(self):
        return len(self.y)
    
    def set_training_mode(self, training):
        """è®¾ç½®æ•°æ®é›†çš„è®­ç»ƒæ¨¡å¼"""
        self.training = training

    def __getitem__(self, idx):
        # è·å–cfDNAæ•°æ®ï¼ˆä¸è¿›è¡Œé¢å¤–å¢å¼ºï¼‰
        cfdna_data = {}
        for i, mod in enumerate(self.Xs):
            data = mod[idx].copy()
            cfdna_data[f'X{i}'] = torch.FloatTensor(data)
        
        sample = cfdna_data
        
        if self.include_ct and self.ct_data is not None:
            ct_slice = self.ct_data[idx].copy()  # å¤åˆ¶æ•°æ®é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            
            # åº”ç”¨CTæ•°æ®å¢å¼ºï¼ˆä»…åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼‰
            if self.training:
                ct_slice = apply_ct_augmentation(ct_slice, training=True, strong_augment=self.strong_augment)
            
            # è½¬æ¢ä¸ºtorch tensoræ ¼å¼ (C, H, W)
            if ct_slice.shape[-1] == 3:  # å¦‚æœæ˜¯ (H, W, C) æ ¼å¼
                ct_slice = np.ascontiguousarray(ct_slice.transpose(2, 0, 1))  # è½¬æ¢ä¸º (C, H, W) å¹¶ç¡®ä¿è¿ç»­
            else:
                ct_slice = np.ascontiguousarray(ct_slice)  # ç¡®ä¿æ•°æ®æ˜¯è¿ç»­çš„
                
            sample['CT'] = torch.FloatTensor(ct_slice)
        
        sample['y'] = torch.tensor(self.y[idx], dtype=torch.long)

        if self.id is not None:
            sample['id'] = str(self.id[idx])
        
        return sample


def create_feature_selectors(train_data_path, modalities, method='combined', k_features=50, 
                           variance_threshold=0.01, lasso_voting_threshold=0.6, lasso_n_runs=10, elastic_l1_ratio=0.7):
    """
    ä¸ºcfDNAæ¨¡æ€åˆ›å»ºç‰¹å¾é€‰æ‹©å™¨
    
    Args:
        train_data_path: è®­ç»ƒæ•°æ®è·¯å¾„
        modalities: cfDNAæ¨¡æ€åˆ—è¡¨
        method: ç‰¹å¾é€‰æ‹©æ–¹æ³• ('variance', 'kbest', 'rfe', 'combined', 'lasso_voting', 'elastic_voting')
        k_features: æ¯ä¸ªæ¨¡æ€ä¿ç•™çš„ç‰¹å¾æ•°é‡
        variance_threshold: æ–¹å·®é˜ˆå€¼ï¼ˆç”¨äºlasso_voting/elastic_votingæ–¹æ³•ï¼‰
        lasso_voting_threshold: æŠ•ç¥¨é˜ˆå€¼
        lasso_n_runs: è¿è¡Œæ¬¡æ•°
        elastic_l1_ratio: ElasticNetçš„L1æ¯”ä¾‹ (0.0=çº¯L2, 1.0=çº¯L1)
    
    Returns:
        feature_selectors: ç‰¹å¾é€‰æ‹©å™¨å­—å…¸
    """
    data = np.load(train_data_path)
    y_train = data['y']
    
    feature_selectors = {}
    
    for mod in modalities:
        X_mod = data[mod]
        # ç®€åŒ–è¾“å‡ºï¼šåªæ˜¾ç¤ºæ¨¡æ€åå’ŒåŸå§‹ç‰¹å¾æ•°
        if len(modalities) <= 4:  # åªåœ¨æ¨¡æ€æ•°é‡å°‘æ—¶æ˜¾ç¤ºè¯¦æƒ…
            print(f"ğŸ”§ {mod}: {X_mod.shape[1]}ç‰¹å¾")
        
        if method == 'variance':
            # æ–¹å·®é˜ˆå€¼é€‰æ‹©
            selector = VarianceThreshold(threshold=0.0)
            
        elif method == 'kbest':
            # å•å˜é‡ç‰¹å¾é€‰æ‹©
            selector = SelectKBest(f_classif, k=min(k_features, X_mod.shape[1]))
            
        elif method == 'rfe':
            # é€’å½’ç‰¹å¾æ¶ˆé™¤
            estimator = RandomForestClassifier(n_estimators=10, random_state=42)
            selector = RFE(estimator, n_features_to_select=min(k_features, X_mod.shape[1]))
            
        elif method == 'combined':
            # ç»„åˆæ–¹æ³•ï¼šå…ˆæ–¹å·®ç­›é€‰ï¼Œå†K-besté€‰æ‹©
            variance_selector = VarianceThreshold(threshold=0.0)
            k_selector = SelectKBest(f_classif, k=min(k_features, X_mod.shape[1]))
            selector = Pipeline([
                ('variance', variance_selector),
                ('k_best', k_selector)
            ])
            
        elif method == 'lasso_voting':
            # ElasticNetæŠ•ç¥¨æ–¹æ³•ï¼šæ–¹å·®è¿‡æ»¤ -> å¤šæ¬¡ElasticNet + æŠ•ç¥¨ -> Topç‰¹å¾
            selector = ElasticNetVotingSelector(
                variance_threshold=variance_threshold,
                n_runs=lasso_n_runs, 
                voting_threshold=lasso_voting_threshold,
                k_features=min(k_features, X_mod.shape[1]),
                l1_ratio=elastic_l1_ratio  # ä½¿ç”¨ä¼ å…¥çš„L1/L2æ··åˆæ¯”ä¾‹
            )
            
        elif method == 'elastic_voting':
            # ElasticNetæŠ•ç¥¨æ–¹æ³•ï¼ˆæ–°æ–¹æ³•åï¼‰ï¼šæ–¹å·®è¿‡æ»¤ -> å¤šæ¬¡ElasticNet + æŠ•ç¥¨ -> Topç‰¹å¾
            selector = ElasticNetVotingSelector(
                variance_threshold=variance_threshold,
                n_runs=lasso_n_runs, 
                voting_threshold=lasso_voting_threshold,
                k_features=min(k_features, X_mod.shape[1]),
                l1_ratio=elastic_l1_ratio  # ä½¿ç”¨ä¼ å…¥çš„L1/L2æ··åˆæ¯”ä¾‹
            )
            
        elif method == 'pca':
            # PCAé™ç»´æ–¹æ³•ï¼šå°†ç‰¹å¾å‹ç¼©åˆ°æŒ‡å®šç»´åº¦
            n_components = min(k_features, X_mod.shape[1], X_mod.shape[0])  # ä¸èƒ½è¶…è¿‡æ ·æœ¬æ•°
            selector = PCA(n_components=n_components, random_state=42)
            
            # è¯¦ç»†è¯´æ˜ç»´åº¦é™åˆ¶
            if X_mod.shape[0] < k_features:
                print(f"âš ï¸  {mod}PCAç»´åº¦å—æ ·æœ¬æ•°é™åˆ¶: æœŸæœ›{k_features}ç»´ â†’ å®é™…{n_components}ç»´ (æ ·æœ¬æ•°={X_mod.shape[0]})")
            else:
                print(f"ğŸ”„ {mod}ä½¿ç”¨PCAé™ç»´: {X_mod.shape[1]:,} -> {n_components} ç»´")
        
        # æ ¹æ®æ–¹æ³•ç±»å‹å†³å®šfitè°ƒç”¨æ–¹å¼
        if method == 'pca':
            # PCAæ˜¯æ— ç›‘ç£æ–¹æ³•ï¼Œä¸éœ€è¦æ ‡ç­¾
            selector.fit(X_mod)
        else:
            # æœ‰ç›‘ç£æ–¹æ³•éœ€è¦æ ‡ç­¾
            selector.fit(X_mod, y_train)
        feature_selectors[mod] = selector
        
        # æ˜¾ç¤ºé€‰æ‹©åçš„ç‰¹å¾æ•°é‡
        X_selected = selector.transform(X_mod)
        # ç®€åŒ–è¾“å‡º
        if len(modalities) <= 4:
            print(f"âœ… {mod}: {X_mod.shape[1]:,} -> {X_selected.shape[1]:,}")
    
    # æ‰“å°æ€»ä½“ç‰¹å¾é€‰æ‹©æ‘˜è¦
    total_original = sum(np.load(train_data_path)[mod].shape[1] for mod in modalities)
    total_selected = sum(selector.transform(np.load(train_data_path)[mod]).shape[1] 
                        for mod, selector in feature_selectors.items())
    reduction_ratio = (1 - total_selected / total_original) * 100
    
    print(f"\nğŸ¯ ç‰¹å¾é€‰æ‹©æ€»ç»“ ({method}):")
    print(f"  ğŸ“Š æ€»ç‰¹å¾æ•°: {total_original:,} -> {total_selected:,}")
    print(f"  ğŸ“‰ å‹ç¼©ç‡: {reduction_ratio:.1f}%")
    print(f"  ğŸ”¢ æ¯æ¨¡æ€å¹³å‡: {total_selected//len(modalities):,} ç‰¹å¾\n")
    
    return feature_selectors


def load_data(data_path, modalities, include_ct=False, feature_selectors=None, training=True):
    """
    åŠ è½½æ•°æ®æ–‡ä»¶å¹¶è¿”å›ç›¸å…³ä¿¡æ¯
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        modalities: æ¨¡æ€åˆ—è¡¨
        include_ct: æ˜¯å¦åŒ…å«CTæ•°æ®
        feature_selectors: cfDNAç‰¹å¾é€‰æ‹©å™¨å­—å…¸
        training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå½±å“CTæ•°æ®å¢å¼ºï¼‰
    
    Returns:
        tuple: (dataset, label_distribution)
    """
    dataset = MultiModalDataset(data_path, modalities, include_ct=include_ct, 
                               feature_selectors=feature_selectors, training=training)
    
    # è®¡ç®—æ ‡ç­¾åˆ†å¸ƒ
    unique, counts = np.unique(dataset.y, return_counts=True)
    label_distribution = (unique, counts)
    
    return dataset, label_distribution


def get_modality_dimensions():
    """è¿”å›å„ä¸ªæ¨¡æ€çš„ç‰¹å¾ç»´åº¦"""
    return {
        'Frag': 888,
        'CNV': 21870,
        'PFE': 19415,
        'NDR': 19434,
        'NDR2K': 19434
    }